{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map2loop: From geology layers to outputs to various 3D modelling programs- Hamersley example (LoopStructural)\n",
    "\n",
    "This notebook reads in three layers from  local or remote sources:  geology polygons, orientation data and fault polylines; and calculates the topological relationships between the different features. Requires compiled cpp code from Vitaliy Ogarko\n",
    "\n",
    "This all gets fed into successive tolopogical and geometric transfroms that end up feeding into geomodeller to make a 3D model \n",
    "<img src='../graphics/map_sm.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a Load libraries and test GDAL path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:34:44.792104Z",
     "start_time": "2020-03-30T06:34:40.574324Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import stat\n",
    "import functools \n",
    "import operator  \n",
    "import matplotlib\n",
    "import networkx as nx\n",
    "import rasterio\n",
    "from shapely.geometry import Polygon\n",
    "from map2loop import m2l_utils\n",
    "from map2loop import m2l_topology\n",
    "from map2loop import m2l_geometry\n",
    "from map2loop import m2l_interpolation\n",
    "from map2loop import m2l_export\n",
    "import time\n",
    "%matplotlib inline\n",
    "t0 = time.time()\n",
    "\n",
    "#newwd=\"C:\\\\Users\\\\00073294\\Dropbox\\\\loop_minex\\\\map2model\\\\\"\n",
    "#os.chdir(newwd)\n",
    "print(\"Current Working Directory \" )\n",
    "\n",
    "gdal_data = os.environ['GDAL_DATA']\n",
    "print(\"***\",gdal_data)\n",
    "print('is dir: ' + str(os.path.isdir(gdal_data)))\n",
    "gcs_csv = os.path.join(gdal_data, 'gcs.csv')\n",
    "print('is file: ' + str(os.path.isfile(gcs_csv)))\n",
    "st = os.stat(gcs_csv)\n",
    "print('is readable: ' + str(bool(st.st_mode & stat.S_IRGRP)))\n",
    "os.environ['PROJ_LIB']=r\"C:\\\\Users\\\\00073294\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\Lib\\\\site-packages\\\\pyproj\\\\proj_dir\\\\share\\\\proj\"\n",
    "print(os.getenv('PROJ_LIB'))\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b Load config file and Create bounding box based on inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:34:44.820997Z",
     "start_time": "2020-03-30T06:34:44.795066Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data_name='test_data3'\n",
    "\n",
    "test_data_path='../'+test_data_name+'/'\n",
    "\n",
    "os.chdir(test_data_path)\n",
    "%run -i \"m2l_config.py\"\n",
    "#%run -i \"m2l_config_remote.py\"\n",
    "print(os.getcwd())\n",
    "\n",
    "bbox2=str(minx)+\",\"+str(miny)+\",\"+str(maxx)+\",\"+str(maxy)\n",
    "lat_point_list = [miny, miny, maxy, maxy, maxy]\n",
    "lon_point_list = [minx, maxx, maxx, minx, minx]\n",
    "bbox_geom = Polygon(zip(lon_point_list, lat_point_list))\n",
    "polygon = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "bbox=(minx,miny,maxx,maxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:34:44.830972Z",
     "start_time": "2020-03-30T06:34:44.824986Z"
    }
   },
   "outputs": [],
   "source": [
    "#local_paths=False\n",
    "fold_decimate=2         \n",
    "\n",
    "contact_decimate=5\n",
    "\n",
    "use_interpolations=True       #use interpolated dips/contacts as additional constraints\n",
    "\n",
    "use_fat=True                   #use fold axial trace orientation hints\n",
    "\n",
    "pluton_form='domes'\n",
    "\n",
    "fault_dip=-999\n",
    "\n",
    "min_fault_length=5000\n",
    "\n",
    "compute_etc=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c First we test to see if we have access to the online data we need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:34:45.276782Z",
     "start_time": "2020-03-30T06:34:44.833963Z"
    }
   },
   "outputs": [],
   "source": [
    "loopwfs=m2l_utils.have_access(\"geo.loop-gis.org\")\n",
    "hawaii=m2l_utils.have_access(\"services.ga.gov.au\")\n",
    "\n",
    "if(not (loopwfs & hawaii)):\n",
    "    local_paths=True\n",
    "    net=False\n",
    "    print('using local paths')\n",
    "else:\n",
    "    net=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d Optional WFS source\n",
    "  \n",
    "WFS brings in field names as lower case, so need to redefine codes too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:34:45.302712Z",
     "start_time": "2020-03-30T06:34:45.281769Z"
    }
   },
   "outputs": [],
   "source": [
    "if((not local_paths) and net):\n",
    "    structure_file='http://geo.loop-gis.org/geoserver/loop/wfs?service=WFS&version=1.1.0&request=GetFeature&typeName=warox_points_f5011&bbox='+bbox2+'&srs=EPSG:28350'\n",
    "    fault_file='http://geo.loop-gis.org/geoserver/loop/wfs?service=WFS&version=1.1.0&request=GetFeature&typeName=linear_500k&bbox='+bbox2+'&srs=EPSG:28350'\n",
    "    geology_file='http://geo.loop-gis.org/geoserver/loop/wfs?service=WFS&version=1.0.0&request=GetFeature&typeName=loop:geol_500k&bbox='+bbox2+'&srs=EPSG:28350'\n",
    "\n",
    "    c_l= {\n",
    "    #Orientations\n",
    "      \"d\": \"dip\",                  #field that contains dip information\n",
    "      \"dd\": \"dip_dir\",             #field that contains dip direction information\n",
    "      \"sf\": 'feature',             #field that contains information on type of structure\n",
    "      \"bedding\": 'Bed',            #text to search for in field defined by sfcode to show that this is a bedding measurement\n",
    "    #Stratigraphy\n",
    "      \"g\": 'group_',               #field that contains coarser stratigraphic coding\n",
    "      \"c\": 'code',                 #field that contains finer stratigraphic coding\n",
    "      \"ds\": 'descriptn',           #field that contains information about lithology\n",
    "      \"u\": 'unitname',             #field that contains alternate stratigraphic coding (not used??)\n",
    "      \"r1\": 'rocktype1',           #field that contains  extra lithology information\n",
    "      \"r2\": 'rocktype2',           #field that contains even more lithology information\n",
    "      \"sill\": 'sill',              #text to search for in field defined by dscode to show that this is a sill\n",
    "      \"intrusive\": 'intrusive',    #text to search for in field defined by dscode to show that this is an intrusion\n",
    "      \"volcanic\": 'volcanic',      #text to search for in field defined by dscode to show that this is an intrusion\n",
    "    #Mineral Deposits\n",
    "      \"msc\": 'SITE_CODE',          #field that contains site code of deposit\n",
    "      \"msn\": 'SHORT_NAME',         #field that contains short name of deposit\n",
    "      \"mst\": 'SITE_TYPE_',         #field that contains site type of deposit\n",
    "      \"mtc\": 'TARGET_COM',         #field that contains target commodity of deposit\n",
    "      \"mscm\": 'SITE_COMMO',        #field that contains site commodity of deposit\n",
    "      \"mcom\": 'COMMODITY_',        #field that contains commodity group of deposit\n",
    "      \"minf\": 'Infrastructure',    #text to search for in field defined by mst code that shows site to ignore\n",
    "    #Timing\n",
    "      \"min\": 'min_age_ma',         #field that contains minimum age of unit defined by ccode\n",
    "      \"max\": 'max_age_ma',         #field that contains maximum age of unit defined by ccode\n",
    "    #faults and folds\n",
    "      \"f\": 'feature',              #field that contains information on type of structure\n",
    "      \"fault\": 'Fault',            #text to search for in field defined by fcode to show that this is a fault\n",
    "      \"fold\": 'Fold axial trace',  #text to search for in field defined by fcode to show that this is a fold axial trace\n",
    "      \"n\": 'name',                 #field that contains information on name of fault (not used??)\n",
    "      \"t\": 'type',                 #field that contains information on type of fold\n",
    "      \"syn\": 'syncline',           #text to search for in field defined by t to show that this is a syncline\n",
    "    #ids\n",
    "      \"o\": 'objectid',             #field that contains unique id of geometry object\n",
    "      \"gi\": 'geopnt_id'            #field that contains unique id of structure point\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1e Plot geology polygons and bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:34:47.228578Z",
     "start_time": "2020-03-30T06:34:45.307699Z"
    }
   },
   "outputs": [],
   "source": [
    "print(geology_file)\n",
    "geology_ll = gpd.read_file(geology_file,bbox=bbox)\n",
    "\n",
    "base=geology_ll.plot(column=c_l['c'],figsize=(10,10),edgecolor='#000000',linewidth=0.2)\n",
    "polygon.plot(ax=base, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1f Save geology to file as WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:34:49.109590Z",
     "start_time": "2020-03-30T06:34:47.232567Z"
    }
   },
   "outputs": [],
   "source": [
    "hint_flag=True # use GSWA strat database to provide relative age hints\n",
    "sub_geol = geology_ll[['geometry', c_l['o'],c_l['c'],c_l['g'],c_l['u'],c_l['min'],c_l['max'],c_l['ds'],c_l['r1'],c_l['r2']]]\n",
    "m2l_topology.save_geol_wkt(sub_geol,geology_file_csv, c_l,hint_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1g1 Save mineral deposits to file as WKT\n",
    "This is not needed by map2loop to build 3D models, but is used by map2model to calculate mineral deposit/topology analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:34:52.837665Z",
     "start_time": "2020-03-30T06:34:49.120561Z"
    }
   },
   "outputs": [],
   "source": [
    "mindep = gpd.read_file(mindep_file,bbox=bbox)\n",
    "\n",
    "sub_mindep = mindep[['geometry', c_l['msc'],c_l['msn'],c_l['mst'],c_l['mtc'],c_l['mscm'],c_l['mcom']]]\n",
    "m2l_topology.save_mindep_wkt(sub_mindep,mindep_file_csv, c_l)\n",
    "\n",
    "base=sub_mindep.plot()\n",
    "polygon.plot(ax=base, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1g2 Read and save WAROX point data as WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:34:54.237914Z",
     "start_time": "2020-03-30T06:34:52.845629Z"
    }
   },
   "outputs": [],
   "source": [
    "warox = gpd.read_file(structure_file,bbox=bbox)\n",
    "\n",
    "sub_pts = warox[['geometry', c_l['gi'],c_l['d'],c_l['dd']]]\n",
    "\n",
    "m2l_topology.save_structure_wkt(sub_pts,structure_file_csv,c_l)\n",
    "\n",
    "base=sub_pts.plot()\n",
    "polygon.plot(ax=base, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1h Plot faults and fold axial traces and bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:34:55.245227Z",
     "start_time": "2020-03-30T06:34:54.244895Z"
    }
   },
   "outputs": [],
   "source": [
    "lines_ll=gpd.read_file(fault_file,bbox=bbox)\n",
    "\n",
    "base2=lines_ll.plot(cmap='rainbow',column=c_l['f'],figsize=(10,10),linewidth=0.4)\n",
    "polygon.plot(ax=base2, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1i Save faults to file as WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:34:55.603272Z",
     "start_time": "2020-03-30T06:34:55.250243Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_lines = lines_ll[['geometry', c_l['o'],c_l['f']]]\n",
    "m2l_topology.save_faults_wkt(sub_lines,fault_file_csv,c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1j Create map2model input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:34:55.626213Z",
     "start_time": "2020-03-30T06:34:55.609257Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_topology.save_Parfile(m2m_cpp_path,c_l,graph_path,geology_file_csv,fault_file_csv,structure_file_csv,minx,maxx,miny,maxy,500.0,'Fe,Cu,Au,NONE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1k Calculate topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:35:04.747886Z",
     "start_time": "2020-03-30T06:34:55.634191Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import platform\n",
    "\n",
    "os.chdir(m2m_cpp_path)\n",
    "print(os.getcwd())\n",
    "\n",
    "#%system ./map2model Parfile\n",
    "if(platform.system()=='Windows'):\n",
    "    subprocess.run([\"map2model.exe\",\"Parfile\"],capture_output=True)\n",
    "else:\n",
    "    subprocess.run([\"./map2model\",\"Parfile\"],capture_output=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1l Simple network graph of the geology with legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:35:07.992233Z",
     "start_time": "2020-03-30T06:35:04.753869Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "G=nx.read_gml(strat_graph_file,label='id')\n",
    "selected_nodes = [n for n,v in G.nodes(data=True) if n >=0]\n",
    "nx.draw_networkx(G, pos=nx.kamada_kawai_layout(G), arrows=True, nodelist=selected_nodes)\n",
    "\n",
    "nlist=list(G.nodes.data('LabelGraphics'))\n",
    "nlist.sort()\n",
    "for no in nlist:\n",
    "    if(no[0]>=0):\n",
    "        elem=str(no[1]).replace(\"{'text':\",\"\").replace(\", 'fontSize': 14}\",\"\")\n",
    "        #second=elem.split(\":\").replace(\"'\",\"\")\n",
    "        print(no[0],\" \",elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## 2a Process topography, stratigraphy, fold axial traces and faults\n",
    "\n",
    "### Takes GML file produced by topology code, combines with geology polygons, structure points and dtm to create 3D model in gempy.<br><br>\n",
    "\n",
    "Limitations:  no dykes, no sills. Sills require us to assign a unique surface to each instance of a sill (sill between units A and B needs to be different from sill of same age and strat codes as one found between E and F). Dykes via cokriging are really hard without just cookie cutting them in (but that is not our problem!). We are not checking for onlap relationships, which can perhaps been seen by having lots of units from one series adjacent to the youngest surface of the older series. Could also think about interpreting these as faults to introduce conceptual uncertainty. All mistakes belong to Mark Jessell, topology code that feeds this system by Vitaliy Ogarko.<br><br>\n",
    "\n",
    "Geology layer needs to have some unique strat code or text, some group code or text to function<br>\n",
    "Structure layer needs dip/dip direction<br>\n",
    "\n",
    "<font color='red'>Currently mostly hardwired to GSWA 500K map so needs work...</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:35:08.010185Z",
     "start_time": "2020-03-30T06:35:08.000212Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('../map2loop')\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:35:08.046091Z",
     "start_time": "2020-03-30T06:35:08.018164Z"
    }
   },
   "outputs": [],
   "source": [
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "sys.path.insert(0,\"../..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "#print(os.getcwd())\n",
    "#os.environ[\"PROJ_LIB\"] = r\"C:\\Users\\00073294\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\pyproj\\proj_dir\\share\\proj\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b Next we define an area of interest and some other basic stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:35:08.080000Z",
     "start_time": "2020-03-30T06:35:08.052074Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "#test_data_path='../test_data3/'\n",
    "\n",
    "\n",
    "#%run -i \"../test_data3/m2l_config.py\"\n",
    "\n",
    "bbox2=str(minx)+\",\"+str(miny)+\",\"+str(maxx)+\",\"+str(maxy)\n",
    "lat_point_list = [miny, miny, maxy, maxy, maxy]\n",
    "lon_point_list = [minx, maxx, maxx, minx, minx]\n",
    "bbox_geom = Polygon(zip(lon_point_list, lat_point_list))\n",
    "polygon = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "bbox=(minx,miny,maxx,maxy)\n",
    "\n",
    "step_out=0.045 #add (in degrees) so edge pixel from dtm reprojection are not found\n",
    "\n",
    "\n",
    "#contact_decimate=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## 2c Download and reproject the appropriate SRTM data\n",
    "mj: Getting this from GA, but could also get from Hawaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:35:08.675417Z",
     "start_time": "2020-03-30T06:35:08.086980Z"
    }
   },
   "outputs": [],
   "source": [
    "polygon_ll=polygon.to_crs(src_crs)\n",
    "\n",
    "minlong=polygon_ll.total_bounds[0]-step_out\n",
    "maxlong=polygon_ll.total_bounds[2]+step_out\n",
    "minlat=polygon_ll.total_bounds[1]-step_out\n",
    "maxlat=polygon_ll.total_bounds[3]+step_out\n",
    "\n",
    "print(minlong,maxlong,minlat,maxlat)\n",
    "if((not local_paths) and net):     \n",
    "    m2l_utils.get_dtm(dtm_file, minlong,maxlong,minlat,maxlat)\n",
    "    geom_rp=m2l_utils.reproject_dtm(dtm_file,dtm_reproj_file,src_crs,dst_crs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## 2d Load stratigraphy graph and create list of series (aka groups)\n",
    "mj: The choice of what constitutes basic unit and what a group of units is hard-wired at the moment, but could be altered to any pair. Not even sure we need two levels but it seemed like a good idea at the time. Note that this needs the arcgis plugin version of the topology code (for now) as it seperates the different sub graphs. Text outputs list alternate topologies for series and surfaces, which if confirmed by comapring max-min ages will be a nice source of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:35:15.555095Z",
     "start_time": "2020-03-30T06:35:08.682393Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groups,glabels,G = m2l_topology.get_series(strat_graph_file,'id')\n",
    "m2l_topology.save_units(G,tmp_path,glabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## 2e Load geology & structure data\n",
    "Currently loading from local files, but could load geology from WFS server at GSWA EXCEPT that the WFS online map has less fields that the zipped shapefiles. Go figure. We don't use fault layer at the moment (except for Vitaliy's topology code) but same logic applies in terms of where to get it from. Already have fault/strat relationships and once we have fault/fault relationships will start to include faults in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:35:16.862576Z",
     "start_time": "2020-03-30T06:35:15.558055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract point data from structure & geology layers for modelling\n",
    "##First we readin the structure and map from shapefiles, or wherever...\n",
    "\n",
    "bbox=(minx,miny,maxx,maxy)\n",
    "geology = gpd.read_file(geology_file,bbox=bbox)\n",
    "\n",
    "\n",
    "structure = gpd.read_file(structure_file,bbox=bbox)\n",
    "structure.crs=dst_crs\n",
    "print(fault_file)\n",
    "faults = gpd.read_file(fault_file,bbox=bbox)\n",
    "faults.crs=dst_crs\n",
    "#display(faults)\n",
    "sub_pts = structure[['geometry',c_l['d'],c_l['dd'],c_l['sf']]] \n",
    "\n",
    "base=geology.plot(column=c_l['c'],figsize=(10,10),edgecolor='#000000',linewidth=0.2)\n",
    "sub_pts.plot(ax=base,edgecolor='black')\n",
    "faults.plot(ax=base, column=c_l['f'],edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## 2f Clip geology, faults, structures and map geology to structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:35:25.066699Z",
     "start_time": "2020-03-30T06:35:16.866568Z"
    }
   },
   "outputs": [],
   "source": [
    "geology = m2l_utils.explode(geology)\n",
    "geology.crs = dst_crs\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "structure_code = gpd.sjoin(sub_pts, geology, how=\"left\", op=\"within\")\n",
    "\n",
    "y_point_list = [miny, miny, maxy, maxy, miny]\n",
    "x_point_list = [minx, maxx, maxx, minx, minx]\n",
    "\n",
    "bbox_geom = Polygon(zip(x_point_list, y_point_list))\n",
    "\n",
    "polygo = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "#display(polygo.geometry)\n",
    "is_bed=structure_code[c_l['sf']].str.contains(c_l['bedding'], regex=False) \n",
    "    \n",
    "all_beds = structure_code[is_bed]\n",
    "#display(sfcode)\n",
    "all_folds=faults[faults[c_l['f']].str.contains(c_l['fold'])]\n",
    "folds_clip=m2l_utils.clip_shp(all_folds,polygo)\n",
    "folds_clip.to_file(tmp_path+'folds_clip.shp')\n",
    "\n",
    "\n",
    "all_faults=faults[faults[c_l['f']].str.contains(c_l['fault'])]\n",
    "\n",
    "\n",
    "\n",
    "#display(structure_code)\n",
    "geol_clip=m2l_utils.clip_shp(geology, polygo)\n",
    "faults_clip=m2l_utils.clip_shp(all_faults,polygo)\n",
    "#display(faults_clip)\n",
    "structure_clip = m2l_utils.clip_shp(all_beds, polygo)\n",
    "\n",
    "base = geol_clip.plot(column=c_l['c'],figsize=(7,7),edgecolor='#000000',linewidth=0.2)\n",
    "faults_clip.plot(ax=base, column=c_l['t'],edgecolor='black')\n",
    "structure_clip.plot(ax=base, column=c_l['c'],edgecolor='black')\n",
    "\n",
    "geol_clip.to_file(tmp_path+'geol_clip.shp')\n",
    "faults_clip.to_file(tmp_path+'faults_clip.shp')\n",
    "structure_clip.to_file(tmp_path+'structure_clip.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2g Create possible stratigraphy sets per group\n",
    "mj: <font color='red'>Uses first of each possible set of toplogies per unit and per group, which is arbitrary. </font>On the other hand we are not checking relative ages again to see if this helps reduce ambiguity, which I think it would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:35:26.532790Z",
     "start_time": "2020-03-30T06:35:25.072683Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2l_topology.save_group(G,tmp_path,glabels,geol_clip,c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2h Display stereonets of bedding by formations and group to see how we can combine them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:35:58.477746Z",
     "start_time": "2020-03-30T06:35:26.553734Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orientations=pd.read_csv(output_path+'orientations.csv',\",\")\n",
    "all_sorts=pd.read_csv(tmp_path+'all_sorts.csv',\",\")\n",
    "            \n",
    "m2l_utils.plot_bedding_stereonets(orientations,all_sorts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## 3a Export orientation data in csv  format\n",
    "mj: Orientation data needs calculated height as file does not provide it, taken from SRTM data already downloaded. To calculate polarity see Cell 6b below for test implementation) \n",
    "\n",
    "Added code to not save intrusion orientation data as they won't have associated surfaces if sill..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:36:00.126352Z",
     "start_time": "2020-03-30T06:35:58.481734Z"
    }
   },
   "outputs": [],
   "source": [
    "dtm = rasterio.open(dtm_reproj_file)\n",
    "m2l_geometry.save_orientations(structure_code,output_path,c_l,orientation_decimate,dtm)\n",
    "m2l_utils.plot_points(output_path+'orientations.csv',geol_clip, 'formation','X','Y',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## 3b Find those series that don't have any orientation or contact point data  then create arbitrary point for series with no orientation data\n",
    "Not sure if gempy needs this but geomodeller does. Currently just gives a point dipping 45 degrees to North, but could use dip direction normal to basal surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:36:01.452813Z",
     "start_time": "2020-03-30T06:36:00.132333Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2l_geometry.create_orientations( tmp_path, output_path, dtm,geol_clip,structure_clip,c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## 4a Export contact information subset of each polygon to gempy format\n",
    "mj: Orientation data needs calculated height as file does not provide it, taken from SRTM data already downloaded. Need to reduce number of points whilst retaining useful info (Ranee's job!)'\n",
    "To calculate which are the basal units contact for a polygon find the polygons which are older than the selected polygon, in the example below the central polygon has relative age 23 so its basal contact is with the polygons whose ages are 26 & 28. If there are no older units for a polygon it has no basal content. We keep every nth node based on the decimate term (simple count along polyline). gempy seems to need at least two points per surface, so we always take the first two points.\n",
    "\n",
    "\n",
    "<img src='../graphics/base.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:36:31.169517Z",
     "start_time": "2020-03-30T06:36:01.460790Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls_dict,ls_dict_decimate=m2l_geometry.save_basal_contacts(tmp_path,dtm,geol_clip,contact_decimate,c_l,intrusion_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b Remove all basal contacts that are defined by faults and save to shapefile (no decimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:36:31.322109Z",
     "start_time": "2020-03-30T06:36:31.177496Z"
    }
   },
   "outputs": [],
   "source": [
    "display(ls_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:36:37.251928Z",
     "start_time": "2020-03-30T06:36:31.331086Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_geometry.save_basal_no_faults(tmp_path+'basal_contacts.shp',tmp_path+'faults_clip.shp',ls_dict,10,c_l,dst_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c Remove faults from decimated basal contacts as save as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:36:47.217345Z",
     "start_time": "2020-03-30T06:36:37.261902Z"
    }
   },
   "outputs": [],
   "source": [
    "contacts=gpd.read_file(tmp_path+'basal_contacts.shp')\n",
    "\n",
    "m2l_geometry.save_basal_contacts_csv(contacts,output_path,dtm,contact_decimate,c_l)\n",
    "\n",
    "m2l_utils.plot_points(output_path+'contacts4.csv',geol_clip, 'formation','X','Y',True)\n",
    "#m2l_geometry.save_contacts_with_faults_removed(tmp_path+'faults_clip.shp',output_path,10,ls_dict,ls_dict_decimate,c_l,dst_crs,dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4d Process fault geometry\n",
    "Save Faults as decimated points and representative orientation  \n",
    "Then, for each  fault string:\n",
    "- incementally advance along polyline every at each inter-node (no point in doing more?)\n",
    "- find local stratigraphy 10m to left and right of fault\n",
    "  \n",
    "Once full fault has been traversed:\n",
    "- Find list of contacts left \n",
    "- Find equivalent contacts on right\n",
    "- use interpolated orientations to estimate minimum true offset assuming vertical displacement and store \n",
    "- if no equivalent found, flag as domain fault and find min strat offset for contact, use cumulative minimum thickness estimate and store with flag (not implemented)\n",
    "- estimate median & sd of minimum fault offset and store with flag (not implemented)\n",
    "\n",
    "Local Orientations\n",
    "Since much of the code is the same, we benefit by calculating local orientation data either side of fault so that geomodeller/gempy have satisfied fault compartment orientation data## Save fault as contact info and and orientation info make vertical (for the moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:40:40.027910Z",
     "start_time": "2020-03-30T06:36:47.229315Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_geometry.save_faults(tmp_path+'faults_clip.shp',output_path,dtm,c_l,fault_decimate,min_fault_length,fault_dip)\n",
    "\n",
    "use_gcode=('Hamersley Group','Fortescue Group','Wyloo Group','Shingle Creek Group','Turee Creek Group' ) \n",
    "use_gcode2=('Hamersley_Group','Fortescue_Group','Wyloo_Group','Shingle_Creek_Group','Turee_Creek_Group' ) \n",
    "\n",
    "m2l_interpolation.process_fault_throw_and_near_orientations(tmp_path,output_path,dtm_reproj_file,c_l,use_gcode,use_gcode2,dst_crs,bbox,scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:40:41.775247Z",
     "start_time": "2020-03-30T06:40:40.033893Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2l_utils.plot_points(output_path+'fault_displacements3.csv',geol_clip, 'apparent_displacement','X','Y',False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4e Process plutons\n",
    "\n",
    "For each instruve but not sill polygon, find older neighbours and store decimated contact points. Also store dipping contact orientations (user defined, just because) with four possible sub-surface configurations:\n",
    "\n",
    "<b>saucers: \\\\_+++_/ <br>\n",
    "batholiths: +++/__ __ _\\\\+++  <br> \n",
    "domes: /‾+++‾\\\\ <br>\n",
    "pendants: +++\\\\_  _/+++ <br>\n",
    "</b>\n",
    "  \n",
    "Saves out orientations and contact points, as well as updated group level stratigraphic column.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:40:54.515265Z",
     "start_time": "2020-03-30T06:40:41.783227Z"
    }
   },
   "outputs": [],
   "source": [
    "bbox=(minx,miny,maxx,maxy)\n",
    "\n",
    "pluton_dip=str(pluton_dip)\n",
    "\n",
    "dist_buffer=10\n",
    "dtm = rasterio.open(dtm_reproj_file)\n",
    "m2l_geometry.process_plutons(tmp_path,output_path,geol_clip,local_paths,dtm,pluton_form,pluton_dip,contact_decimate,c_l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a Interpolates a regular grid of orientations from an  shapefile of arbitrarily-located points and saves out four csv files of l,m & n direction cosines and dip dip direction data\n",
    "\n",
    "Can choose between various RBF and IDW options   \n",
    "  \n",
    "The purpose of these interpolations and associated code is to help in three cases:\n",
    "- Providing estimated dips and contacts in fault-bounded domains where no structural data are available\n",
    "- Needed to estimate true thickness of formations\n",
    "- Possibly useful for poulating parts of maps where little structural data is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:40:54.530226Z",
     "start_time": "2020-03-30T06:40:54.521251Z"
    }
   },
   "outputs": [],
   "source": [
    "use_gcode=('Hamersley Group','Fortescue Group','Wyloo Group','Shingle Creek Group','Turee Creek Group' ) ################# MOVE UP   #########################\n",
    "structure_file=tmp_path+'structure_clip.shp'\n",
    "bbox=(minx+inset,miny+inset,maxx-inset,maxy-inset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:40:54.552171Z",
     "start_time": "2020-03-30T06:40:54.536211Z"
    }
   },
   "outputs": [],
   "source": [
    "print(c_l['g'])\n",
    "print(use_gcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:40:58.765931Z",
     "start_time": "2020-03-30T06:40:54.558154Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_interpolation.interpolate_orientations(structure_file,tmp_path,bbox,c_l,use_gcode,scheme,gridx,gridy,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b Interpolates a regular grid from a shapefile of from contacts and saves out three csv files of l & m direction cosines and strike data\n",
    "\n",
    "Can choose between various RBF and IDW options  \n",
    "\n",
    "<font face color='red'>Best with basal contacts only, using Notebook #1 as this avoids intrusive  and fault contacts being considered</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:40:58.791859Z",
     "start_time": "2020-03-30T06:40:58.775902Z"
    }
   },
   "outputs": [],
   "source": [
    "geology_file=tmp_path+'basal_contacts.shp'\n",
    "\n",
    "dtm = rasterio.open(dtm_reproj_file)\n",
    "use_gcode=('Hamersley_Group','Fortescue_Group','Wyloo_Group','Shingle_Creek_Group','Turee_Creek_Group' ) ################# MOVE UP   #########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:42:32.098657Z",
     "start_time": "2020-03-30T06:40:58.799838Z"
    }
   },
   "outputs": [],
   "source": [
    "contact_decimate=2\n",
    "null_scheme='null'\n",
    "\n",
    "m2l_interpolation.save_contact_vectors(geology_file,tmp_path,dtm,bbox,c_l,null_scheme,contact_decimate)\n",
    "\n",
    "m2l_interpolation.interpolate_contacts(geology_file,tmp_path,dtm,bbox,c_l,use_gcode,scheme,gridx,gridy,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c Combines rbf of contact orientations with rbf of dip/dipdir measurements\n",
    "\n",
    "Uses normalised direction cosines **(l,m,n)**:  \n",
    "- **l,m** from RBF of basal contact orientations  \n",
    "- **signs of l & m** from misorientation with RBF of orientation data and  \n",
    "- **n** from RBF of orientation data  \n",
    "  \n",
    "May be useful for adding data where no orientations are available (e.g. in fault bounded domains) and for calculating true thickness of layers. Assumes a 2D plane of data, but if 3D RBF was calulated and projected contact info was used it should apply with topography too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:42:32.386918Z",
     "start_time": "2020-03-30T06:42:32.103641Z"
    }
   },
   "outputs": [],
   "source": [
    "geology_file=tmp_path+'geol_clip.shp'\n",
    "combo_file=tmp_path+'combo.csv'\n",
    "\n",
    "lc=np.loadtxt(tmp_path+'interpolation_contacts_l.csv',skiprows =1,delimiter =',',dtype=float)\n",
    "mc=np.loadtxt(tmp_path+'interpolation_contacts_m.csv',skiprows =1,delimiter =',',dtype=float)\n",
    "lo=np.loadtxt(tmp_path+'interpolation_l.csv',skiprows =1,delimiter =',',dtype=float)\n",
    "mo=np.loadtxt(tmp_path+'interpolation_m.csv',skiprows =1,delimiter =',',dtype=float)\n",
    "no=np.loadtxt(tmp_path+'interpolation_n.csv',skiprows =1,delimiter =',',dtype=float)\n",
    "xy=np.loadtxt(tmp_path+'interpolation_'+scheme+'.csv',skiprows =1,delimiter =',',dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:42:41.019859Z",
     "start_time": "2020-03-30T06:42:32.390877Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_interpolation.join_contacts_and_orientations(combo_file,geology_file,tmp_path,dtm_reproj_file,c_l,lo,mo,no,lc,mc,xy,dst_crs,bbox,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5d Estimate formation thickness and normalised formation thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:47:55.642725Z",
     "start_time": "2020-03-30T06:42:41.025843Z"
    }
   },
   "outputs": [],
   "source": [
    "buffer =5000\n",
    "max_thickness_allowed=10000\n",
    "\n",
    "m2l_geometry.calc_thickness(tmp_path,output_path,buffer,max_thickness_allowed,c_l)\n",
    "\n",
    "m2l_geometry.normalise_thickness(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:47:58.106153Z",
     "start_time": "2020-03-30T06:47:55.647712Z"
    }
   },
   "outputs": [],
   "source": [
    "    \n",
    "m2l_utils.plot_points(output_path+'formation_thicknesses_norm.csv',geol_clip,'norm_th','x','y',False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5e Creates fold axial trace points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:48:07.675628Z",
     "start_time": "2020-03-30T06:47:58.112136Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_geometry.save_fold_axial_traces(tmp_path+'folds_clip.shp',output_path,dtm,c_l,fold_decimate)\n",
    "\n",
    "#Save fold axial trace near-hinge orientations\n",
    "fat_step=750         # how much to step out normal to fold axial trace\n",
    "close_dip=20.0       #dip to assign to all new orientations\n",
    "   \n",
    "m2l_geometry.save_fold_axial_traces_orientations(tmp_path+'folds_clip.shp',output_path,tmp_path,dtm,c_l,dst_crs,fold_decimate,fat_step,close_dip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6a Preprocess data to ensure it meets modelling requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:48:28.259180Z",
     "start_time": "2020-03-30T06:48:07.680616Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(local_paths): ###############FUDGE#############\n",
    "    use_gcode=('Hamersley_Group','Fortescue_Group','Turee_Creek_Group','A_mgn_PRK',  'A_mgn_PMI' ) ################# MOVE UP   #########################\n",
    "    #use_gcode=('Hamersley_Group','Fortescue_Group','Wyloo_Group','Shingle_Creek_Group','Turee_Creek_Group','A_mgn_PRK',  'A_mgn_PMI' ) ################# MOVE UP   #########################\n",
    "else:\n",
    "    use_gcode=('Hamersley_Group','Fortescue_Group','Wyloo_Group','Shingle_Creek_Group','Turee_Creek_Group','A_mgn_PMI',  'A_mgn_PRK' ) ################# MOVE UP   #########################\n",
    "\n",
    "\n",
    "print('only processing',use_gcode)\n",
    "\n",
    "#inputs=('invented_orientations','interpolated_orientations','intrusive_orientations','fat_orientations','near_fault_orientations')\n",
    "#inputs=('invented_orientations','interpolated_orientations','intrusive_orientations','fat_orientations','near_fault_orientations')\n",
    "inputs=('invented_orientations','fat_orientations')\n",
    "#inputs=('invented_orientations')\n",
    "\n",
    "m2l_geometry.tidy_data(output_path,tmp_path,use_gcode,use_interpolations,use_fat,pluton_form,inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b Calculate polarity of original bedding orientation data (not used yet in final calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:49:43.072066Z",
     "start_time": "2020-03-30T06:48:28.269154Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2l_geometry.save_orientations_with_polarity(output_path+'orientations.csv',output_path,c_l,tmp_path+'basal_contacts.shp',tmp_path+'all_sorts.csv',)\n",
    "\n",
    "m2l_utils.plot_points(output_path+'orientations_polarity.csv',geol_clip,'polarity','X','Y',True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6c Calculate minimum fault offset from stratigraphy and stratigraphic fault offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:50:03.304130Z",
     "start_time": "2020-03-30T06:49:43.082035Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2l_geometry.fault_strat_offset(output_path,c_l,dst_crs,output_path+'formation_summary_thicknesses.csv', tmp_path+'all_sorts.csv',tmp_path+'faults_clip.shp',tmp_path+'geol_clip.shp',output_path+'fault_dimensions.csv')\n",
    "\n",
    "\n",
    "m2l_utils.plot_points(output_path+'fault_strat_offset3.csv',geol_clip,'min_offset','X','Y',True)\n",
    "m2l_utils.plot_points(output_path+'fault_strat_offset3.csv',geol_clip,'strat_offset','X','Y',True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:33:14.396763Z",
     "start_time": "2020-03-30T06:33:14.287052Z"
    }
   },
   "source": [
    "## 6d Calculate igneous intrusion polarity (local age gradient) information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:50:52.631601Z",
     "start_time": "2020-03-30T06:50:34.223659Z"
    }
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import shape, Polygon, LineString, Point\n",
    "from math import sin, cos, atan, atan2, asin, radians, degrees, sqrt, pow, acos, fabs, tan, fmod\n",
    "\n",
    "def process_plutons_polarity(tmp_path,output_path,geol_clip,local_paths,dtm,pluton_form,pluton_dip,contact_decimate,c_l, calc_lag):\n",
    "    \n",
    "    groups=np.genfromtxt(tmp_path+'groups.csv',delimiter=',',dtype='U25')\n",
    "    ngroups=len(groups[0])-1\n",
    "\n",
    "    orig_ngroups=ngroups\n",
    "\n",
    "    gp_ages=np.zeros((1000,3))\n",
    "    gp_names=np.zeros((1000),dtype='U25')\n",
    "\n",
    "    for i in range (0,ngroups):\n",
    "        gp_ages[i,0]=-1e6 # group max_age\n",
    "        gp_ages[i,1]=1e6 # group min_age\n",
    "        gp_ages[i,2]=i # group index\n",
    "        gp_names[i]=groups[0][i+1].replace(\"\\n\",\"\")\n",
    "        #print(i,gp_names[i])\n",
    "\n",
    "    #print(local_paths)  \n",
    "\n",
    "    allc=open(output_path+'all_ign_contacts.csv',\"w\")\n",
    "    allc.write('GROUP_,id,x,y,z,code\\n')\n",
    "    ac=open(output_path+'ign_contacts.csv',\"w\")\n",
    "    ac.write(\"X,Y,Z,formation\\n\")\n",
    "    ao=open(output_path+'ign_orientations_'+pluton_form+'.csv',\"w\")\n",
    "    ao.write(\"X,Y,Z,azimuth,dip,polarity,formation\\n\")\n",
    "    if(calc_lag==True):\n",
    "        ap=open(output_path+'ign_lag.csv',\"w\")\n",
    "        ap.write(\"X,Y,Z,azimuth,formation\\n\")\n",
    "    #print(output_path+'ign_orientations_'+pluton_form+'.csv')\n",
    "    j=0\n",
    "    allpts=0\n",
    "    ls_dict={}\n",
    "    ls_dict_decimate={}\n",
    "    id=0\n",
    "    for ageol in geol_clip.iterrows(): \n",
    "        ades=str(ageol[1][c_l['ds']])\n",
    "        arck=str(ageol[1][c_l['r1']])\n",
    "        if(str(ageol[1][c_l['g']])=='None'):\n",
    "            agroup=str(ageol[1][c_l['c']])\n",
    "        else:\n",
    "            agroup=str(ageol[1][c_l['g']])\n",
    "        \n",
    "        for i in range(0,ngroups):\n",
    "            if (gp_names[i]==agroup):\n",
    "                if(int(ageol[1][c_l['max']]) > gp_ages[i][0]  ):\n",
    "                    gp_ages[i][0] = ageol[1][c_l['max']]\n",
    "                if(int(ageol[1][c_l['min']]) < gp_ages[i][1]  ):\n",
    "                    gp_ages[i][1] = ageol[1][c_l['min']]\n",
    "        if(c_l['intrusive'] in arck and c_l['sill'] not in ades):\n",
    "            newgp=str(ageol[1][c_l['c']])\n",
    "            #print(newgp)\n",
    "            if(str(ageol[1][c_l['g']])=='None'):\n",
    "                agp=str(ageol[1][c_l['c']])\n",
    "            else:\n",
    "                agp=str(ageol[1][c_l['g']])\n",
    "\n",
    "            if(not newgp  in gp_names):\n",
    "                gp_names[ngroups]=newgp\n",
    "                gp_ages[ngroups][0]=ageol[1][c_l['max']]\n",
    "                gp_ages[ngroups][1]=ageol[1][c_l['min']]\n",
    "                gp_ages[ngroups][2]=ngroups\n",
    "                ngroups=ngroups+1\n",
    "                \n",
    "            neighbours=[]\n",
    "            j+=1\n",
    "            central_age=ageol[1][c_l['min']]    #absolute age of central polygon\n",
    "            central_poly=ageol[1].geometry\n",
    "            for bgeol in geol_clip.iterrows(): #potential neighbouring polygons  \n",
    "                if(ageol[1].geometry!=bgeol[1].geometry): #do not compare with self\n",
    "                    if (ageol[1].geometry.intersects(bgeol[1].geometry)): # is a neighbour\n",
    "                        neighbours.append([(bgeol[1][c_l['c']],bgeol[1][c_l['min']],bgeol[1][c_l['r1']],bgeol[1][c_l['ds']],bgeol[1].geometry)])  \n",
    "            #display(neighbours)\n",
    "            if(len(neighbours) >0):\n",
    "                for i in range (0,len(neighbours)):\n",
    "                    if((c_l['intrusive'] in neighbours[i][0][2] and c_l['sill'] not in ades) \n",
    "                       #or ('intrusive' not in neighbours[i][0][2]) and neighbours[i][0][1] > central_age ): # neighbour is older than central\n",
    "                       or (c_l['intrusive'] not in neighbours[i][0][2]) and neighbours[i][0][1]  ): # neighbour is older than central\n",
    "                        older_polygon=neighbours[i][0][4]\n",
    "                        if(not central_poly.is_valid ):\n",
    "                            central_poly = central_poly.buffer(0)\n",
    "                        if(not older_polygon.is_valid):\n",
    "                            older_polygon = older_polygon.buffer(0)\n",
    "                        LineStringC = central_poly.intersection(older_polygon)\n",
    "                        if(LineStringC.wkt.split(\" \")[0]=='GEOMETRYCOLLECTION' or \n",
    "                           LineStringC.wkt.split(\" \")[0]=='MULTIPOLYGON' or\n",
    "                           LineStringC.wkt.split(\" \")[0]=='POLYGON'): #ignore polygon intersections for now, worry about them later!\n",
    "                            #print(\"debug:GC,MP,P\")\n",
    "                            continue\n",
    "\n",
    "                        elif(LineStringC.wkt.split(\" \")[0]=='MULTILINESTRING'):\n",
    "                            k=0\n",
    "                            ls_dict[id] = {\"id\": id,c_l['c']:newgp,c_l['g']:newgp, \"geometry\": LineStringC}\n",
    "                            id=id+1\n",
    "                            for lineC in LineStringC: #process all linestrings\n",
    "                                if(m2l_utils.mod_safe(k,contact_decimate)==0 or k==int((len(LineStringC)-1)/2) or k==len(LineStringC)-1): #decimate to reduce number of points, but also take second and third point of a series to keep gempy happy\n",
    "                                    locations=[(lineC.coords[0][0],lineC.coords[0][1])] #doesn't like point right on edge?\n",
    "                                    if(lineC.coords[0][0] > dtm.bounds[0] and lineC.coords[0][0] < dtm.bounds[2] and  \n",
    "                                       lineC.coords[0][1] > dtm.bounds[1] and lineC.coords[0][1] < dtm.bounds[3]):       \n",
    "                                            height=m2l_utils.value_from_raster(dtm,locations)\n",
    "                                            ostr=str(lineC.coords[0][0])+\",\"+str(lineC.coords[0][1])+\",\"+height+\",\"+newgp.replace(\" \",\"_\").replace(\"-\",\"_\")+\"\\n\"\n",
    "                                            ac.write(ostr)\n",
    "                                            allc.write(agp+\",\"+str(ageol[1][c_l['o']])+\",\"+ostr)\n",
    "                                            ls_dict_decimate[allpts] = {\"id\": allpts,c_l['c']:newgp,c_l['g']:newgp, \"geometry\": Point(lineC.coords[0][0],lineC.coords[0][1])}\n",
    "                                            allpts+=1 \n",
    "                                    else:\n",
    "                                        continue\n",
    "                                else:\n",
    "                                    if(lineC.coords[0][0] > dtm.bounds[0] and lineC.coords[0][0] < dtm.bounds[2] and  \n",
    "                                            lineC.coords[0][1] > dtm.bounds[1] and lineC.coords[0][1] < dtm.bounds[3]):       \n",
    "                                        height=m2l_utils.value_from_raster(dtm,locations)\n",
    "                                        ostr=str(lineC.coords[0][0])+\",\"+str(lineC.coords[0][1])+\",\"+height+\",\"+newgp.replace(\" \",\"_\").replace(\"-\",\"_\")+\"\\n\"\n",
    "                                        #ls_dict_decimate[allpts] = {\"id\": id,\"CODE\":ageol[1]['CODE'],\"GROUP_\":ageol[1]['GROUP_'], \"geometry\": Point(lineC.coords[0][0],lineC.coords[0][1])}\n",
    "                                        allc.write(agp+\",\"+str(ageol[1][c_l['o']])+\",\"+ostr)\n",
    "                                        allpts+=1\n",
    "                                \n",
    "                                if(m2l_utils.mod_safe(k,contact_decimate)==0 or k==int((len(LineStringC)-1)/2) or k==len(LineStringC)-1): #decimate to reduce number of points, but also take second and third point of a series to keep gempy happy\n",
    "                                    dlsx=lineC.coords[0][0]-lineC.coords[1][0]\n",
    "                                    dlsy=lineC.coords[0][1]-lineC.coords[1][1]\n",
    "                                    lsx=dlsx/sqrt((dlsx*dlsx)+(dlsy*dlsy))\n",
    "                                    lsy=dlsy/sqrt((dlsx*dlsx)+(dlsy*dlsy))                                        \n",
    "\n",
    "                                    locations=[(lineC.coords[0][0],lineC.coords[0][1])]\n",
    "                                    height= m2l_utils.value_from_raster(dtm,locations)\n",
    "                                    azimuth=(180+degrees(atan2(lsy,-lsx)))%360 #normal to line segment\n",
    "                                    testpx=lineC.coords[0][0]+lsy # pt just a bit in/out from line\n",
    "                                    testpy=lineC.coords[0][0]+lsx\n",
    "\n",
    "                                    for cgeol in geol_clip.iterrows(): # check on direction to dip\n",
    "                                        if LineString(central_poly.exterior.coords).contains(Point(testpx, testpy)):\n",
    "                                            azimuth=(azimuth-180)%360\n",
    "                                            break\n",
    "                                    if(pluton_form=='saucers'):\n",
    "                                        ostr=str(lineC.coords[0][0])+\",\"+str(lineC.coords[0][1])+\",\"+str(height)+\",\"+str(azimuth)+\",\"+str(pluton_dip)+\",1,\"+newgp.replace(\" \",\"_\").replace(\"-\",\"_\")+\"\\n\"\n",
    "                                    elif(pluton_form=='domes'):\n",
    "                                        azimuth=(azimuth-180)%360\n",
    "                                        ostr=str(lineC.coords[0][0])+\",\"+str(lineC.coords[0][1])+\",\"+str(height)+\",\"+str(azimuth)+\",\"+str(pluton_dip)+\",0,\"+newgp.replace(\" \",\"_\").replace(\"-\",\"_\")+\"\\n\"\n",
    "                                    elif(pluton_form=='pendant'):\n",
    "                                        ostr=str(lineC.coords[0][0])+\",\"+str(lineC.coords[0][1])+\",\"+str(height)+\",\"+str(azimuth)+\",\"+str(pluton_dip)+\",0,\"+newgp.replace(\" \",\"_\").replace(\"-\",\"_\")+\"\\n\"\n",
    "                                    else: #pluton_form == batholith\n",
    "                                        azimuth=(azimuth-180)%360\n",
    "                                        ostr=str(lineC.coords[0][0])+\",\"+str(lineC.coords[0][1])+\",\"+str(height)+\",\"+str(azimuth)+\",\"+str(pluton_dip)+\",1,\"+newgp.replace(\" \",\"_\").replace(\"-\",\"_\")+\"\\n\"\n",
    "                                                                            \n",
    "                                    ao.write(ostr)\n",
    "                                    \n",
    "                                    if(calc_lag==True):                                    \n",
    "                                        ostr=str(lineC.coords[0][0]+lsx)+\",\"+str(lineC.coords[0][1]+lsy)+\",\"+str(height)+\",\"+str((azimuth+180)%360)+\",\"+newgp.replace(\" \",\"_\").replace(\"-\",\"_\")+\"\\n\"\n",
    "                                        ap.write(ostr)\n",
    "\n",
    "                                k+=1\n",
    "                        elif(LineStringC.wkt.split(\" \")[0]=='LINESTRING'): # apparently this is not needed\n",
    "                            #print(\"debug:LINESTRING\")\n",
    "                            k=0\n",
    "                            for pt in LineStringC.coords: #process one linestring\n",
    "                                k+=1\n",
    "                        elif(LineStringC.wkt.split(\" \")[0]=='POINT'): # apparently this is not needed\n",
    "                            #print(\"debug:POINT\")\n",
    "                            k+=1\n",
    "                        else:\n",
    "                            #print(LineStringC.wkt.split(\" \")[0]) # apparently this is not needed\n",
    "                            k+=1\n",
    "    ac.close()\n",
    "    ao.close()\n",
    "    if(calc_lag==True):\n",
    "        ap.close()\n",
    "    allc.close()\n",
    "\n",
    "      \n",
    "    an=open(tmp_path+'groups2.csv',\"w\")\n",
    "\n",
    "    for i in range (0,orig_ngroups):\n",
    "        print(i,gp_names[i].replace(\" \",\"_\").replace(\"-\",\"_\"))\n",
    "        an.write(gp_names[i].replace(\" \",\"_\").replace(\"-\",\"_\")+'\\n')\n",
    "    an.close()\n",
    "\n",
    "    all_sorts=pd.read_csv(tmp_path+'all_sorts.csv',\",\")\n",
    "\n",
    "    as_2=open(tmp_path+'all_sorts.csv',\"r\")\n",
    "    contents =as_2.readlines()\n",
    "    as_2.close\n",
    "\n",
    "    all_sorts_file=open(tmp_path+'all_sorts2.csv',\"w\")\n",
    "    all_sorts_file.write('index,group number,index in group,number in group,code,group\\n')\n",
    "    j=1\n",
    "\n",
    "    for i in range(1,len(all_sorts)+1):    \n",
    "        all_sorts_file.write(contents[i]) #don't write out if already there in new groups list#\n",
    "        \n",
    "    all_sorts_file.close()\n",
    "    print('pluton contacts and orientations saved as:')\n",
    "    print(output_path+'ign_contacts.csv')\n",
    "    print(output_path+'ign_orientations_'+pluton_form+'.csv')\n",
    "    if(calc_lag==True):\n",
    "        print(output_path+'ign_lag.csv')\n",
    "process_plutons_polarity(tmp_path,output_path,geol_clip,local_paths,dtm,pluton_form,pluton_dip,contact_decimate,c_l,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6e Save near contact  polarity (local age gradeint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_basal_contact_polarity(path_in,dtm,geol_clip,contact_decimate,c_l,intrusion_mode, calc_lag):\n",
    "    #print(\"decimation: 1 /\",contact_decimate)\n",
    "    plist=[]\n",
    "    i=0\n",
    "    all_geom=m2l_utils.explode(geol_clip)\n",
    "\n",
    "\n",
    "    for ageol in all_geom.iterrows(): # central polygon\n",
    "        all_coords=extract_poly_coords(ageol[1].geometry,0)\n",
    "        plist+=(i,list(all_coords['exterior_coords']),ageol[1][c_l['c']],ageol[1][c_l['ds']],ageol[1][c_l['g']],ageol[1][c_l['r1']],ageol[1][c_l['o']])\n",
    "        i=i+1\n",
    "        for j in range(0,len(all_coords['interior_coords']),2):\n",
    "            plist+=(i,list(all_coords['interior_coords'][j+1]),ageol[1][c_l['c']],ageol[1][c_l['ds']],ageol[1][c_l['g']],ageol[1][c_l['r1']],ageol[1][c_l['o']])\n",
    "            i=i+1\n",
    "               \n",
    "    #dataset = rasterio.open(path_in+'/dtm_rp.tif')\n",
    "    ag=open(path_in+'/all_sorts.csv',\"r\")\n",
    "    contents =ag.readlines()\n",
    "    ag.close\n",
    "    #print(\"surfaces:\",len(contents))\n",
    "    #print(\"polygons:\",len(all_geom))\n",
    "    ulist=[]\n",
    "    for i in range(1,len(contents)):\n",
    "        #print(contents[i].replace(\"\\n\",\"\"))\n",
    "        cont_list=contents[i].split(\",\")\n",
    "        ulist.append([i, cont_list[4].replace(\"\\n\",\"\")])\n",
    "    #print(ulist)\n",
    "\n",
    "    allc=open(path_in+'/all_contacts.csv',\"w\")\n",
    "    allc.write('GROUP_,id,x,y,z,code\\n')\n",
    "    ac=open(path_in+'/contacts.csv',\"w\")\n",
    "    ac.write(\"X,Y,Z,formation\\n\")\n",
    "    #print(dtm.bounds)\n",
    "    j=0\n",
    "    allpts=0\n",
    "    deci_points=0\n",
    "    ls_dict={}\n",
    "    ls_dict_decimate={}\n",
    "    id=0\n",
    "    #print(len(plist))\n",
    "    for a_poly in range(0,len(plist),7):\n",
    "        if( not 'intrusive' in plist[a_poly+5]):\n",
    "            a_polygon=Polygon(plist[a_poly+1])\n",
    "            agp=str(plist[a_poly+4])\n",
    "            if(agp=='None'):\n",
    "                agp=plist[a_poly+2].replace(\" \",\"_\").replace(\"-\",\"_\")\n",
    "\n",
    "            neighbours=[]\n",
    "            j+=1\n",
    "            out=[item for item in ulist if plist[a_poly+2].replace(\" \",\"_\").replace(\"-\",\"_\") in item]\n",
    "            if(len(out)>0):\n",
    "                central=out[0][0]    #relative age of central polygon\n",
    "\n",
    "                for b_poly in range(0,len(plist),7):\n",
    "                    b_polygon=LineString(plist[b_poly+1])\n",
    "\n",
    "                    if(plist[a_poly] != plist[b_poly]): #do not compare with self\n",
    "\n",
    "                        if (a_polygon.intersects(b_polygon)) : # is a neighbour, but not a sill\n",
    "                            if(  (not c_l['sill'] in plist[b_poly+3] or not c_l['intrusive'] in plist[b_poly+5]) and intrusion_mode==0): #intrusion_mode=0 (sills only excluded)\n",
    "                                 neighbours.append((b_poly))                               \n",
    "                            elif((not c_l['intrusive'] in plist[b_poly+5])  and intrusion_mode==1): #intrusion_mode=1 (all intrusions  excluded)\n",
    "                                 neighbours.append((b_poly))                               \n",
    "\n",
    "\n",
    "                if(len(neighbours) >0):\n",
    "                    for i in range (0,len(neighbours)):\n",
    "                        b_polygon=LineString(plist[neighbours[i]+1])\n",
    "\n",
    "                        out=[item for item in ulist if plist[neighbours[i]+2].replace(\" \",\"_\").replace(\"-\",\"_\")  in item]\n",
    "\n",
    "                        if(len(out)>0):\n",
    "                            #if(out[0][0] > central and out[0][0] < youngest_older): # neighbour is older than central, and younger than previous candidate\n",
    "                            if(out[0][0] > central  ): # neighbour is older than central\n",
    "\n",
    "                                if(not a_polygon.is_valid ):\n",
    "                                    a_polygon = a_polygon.buffer(0)\n",
    "                                if(not b_polygon.is_valid):\n",
    "                                    b_polygon = b_polygon.buffer(0)                                    \n",
    "                                LineStringC = a_polygon.intersection(b_polygon)\n",
    "\n",
    "                                if(LineStringC.wkt.split(\" \")[0]=='GEOMETRYCOLLECTION' ): #ignore weird intersections for now, worry about them later!\n",
    "                                    #print(\"debug:GC\")\n",
    "                                    continue\n",
    "                                elif(LineStringC.wkt.split(\" \")[0]=='MULTIPOLYGON' or\n",
    "                                     LineStringC.wkt.split(\" \")[0]=='POLYGON'):\n",
    "                                         print(\"debug:MP,P\",ageol[1][c_l['c']])\n",
    "\n",
    "                                elif(LineStringC.wkt.split(\" \")[0]=='MULTILINESTRING'):\n",
    "                                    k=0\n",
    "\n",
    "                                    if(str(plist[a_poly+4])=='None'):\n",
    "                                        ls_dict[id] = {\"id\": id,c_l['c']:plist[a_poly+2].replace(\" \",\"_\").replace(\"-\",\"_\"),c_l['g']:plist[a_poly+2].replace(\" \",\"_\").replace(\"-\",\"_\"), \"geometry\": LineStringC}\n",
    "                                    else:\n",
    "                                        ls_dict[id] = {\"id\": id,c_l['c']:plist[a_poly+2].replace(\" \",\"_\").replace(\"-\",\"_\"),c_l['g']:plist[a_poly+4].replace(\" \",\"_\").replace(\"-\",\"_\"), \"geometry\": LineStringC}\n",
    "                                    id=id+1\n",
    "                                    for lineC in LineStringC: #process all linestrings\n",
    "                                        if(m2l_utils.mod_safe(k,contact_decimate)==0 or k==int((len(LineStringC)-1)/2) or k==len(LineStringC)-1): #decimate to reduce number of points, but also take second and third point of a series to keep gempy happy\n",
    "                                            locations=[(lineC.coords[0][0],lineC.coords[0][1])] #doesn't like point right on edge?\n",
    "                                            if(lineC.coords[0][0] > dtm.bounds[0] and lineC.coords[0][0] < dtm.bounds[2] and  \n",
    "                                               lineC.coords[0][1] > dtm.bounds[1] and lineC.coords[0][1] < dtm.bounds[3]):       \n",
    "                                                    height=m2l_utils.value_from_raster(dtm,locations)\n",
    "                                                    ostr=str(lineC.coords[0][0])+\",\"+str(lineC.coords[0][1])+\",\"+height+\",\"+str(plist[a_poly+2].replace(\" \",\"_\").replace(\"-\",\"_\"))+\"\\n\"\n",
    "                                                    ac.write(ostr)\n",
    "                                                    allc.write(agp+\",\"+str(ageol[1][c_l['o']])+\",\"+ostr)\n",
    "                                                    if(str(plist[a_poly+4])=='None'):\n",
    "                                                        ls_dict_decimate[deci_points] = {\"id\": allpts,c_l['c']:plist[a_poly+2].replace(\" \",\"_\").replace(\"-\",\"_\"),c_l['g']:plist[a_poly+2].replace(\" \",\"_\").replace(\"-\",\"_\"), \"geometry\": Point(lineC.coords[0][0],lineC.coords[0][1])}\n",
    "                                                    else:\n",
    "                                                        ls_dict_decimate[deci_points] = {\"id\": allpts,c_l['c']:plist[a_poly+2].replace(\" \",\"_\").replace(\"-\",\"_\"),c_l['g']:plist[a_poly+4].replace(\" \",\"_\").replace(\"-\",\"_\"), \"geometry\": Point(lineC.coords[0][0],lineC.coords[0][1])}\n",
    "                                                    allpts+=1 \n",
    "                                                    deci_points=deci_points+1\n",
    "                                            else:\n",
    "                                                continue\n",
    "                                                #print(\"debug:edge points\")\n",
    "                                        else:\n",
    "                                            locations=[(lineC.coords[0][0]+0.0000001,lineC.coords[0][1])] #doesn't like point right on edge?\n",
    "                                            if(lineC.coords[0][0] > dtm.bounds[0] and lineC.coords[0][0] < dtm.bounds[2] and  \n",
    "                                                lineC.coords[0][1] > dtm.bounds[1] and lineC.coords[0][1] < dtm.bounds[3]):       \n",
    "                                                height=m2l_utils.value_from_raster(dtm,locations)\n",
    "                                                ostr=str(lineC.coords[0][0])+\",\"+str(lineC.coords[0][1])+\",\"+height+\",\"+str(plist[a_poly+2].replace(\" \",\"_\").replace(\"-\",\"_\"))+\"\\n\"\n",
    "                                                allc.write(agp+\",\"+str(ageol[1][c_l['o']])+\",\"+ostr)\n",
    "                                                allpts+=1    \n",
    "                                        k+=1\n",
    "                                elif(LineStringC.wkt.split(\" \")[0]=='LINESTRING'): # apparently this is not needed\n",
    "                                    k=0\n",
    "                                    for pt in LineStringC.coords: #process one linestring\n",
    "                                        k+=1\n",
    "                                elif(LineStringC.wkt.split(\" \")[0]=='POINT'): # apparently this is not needed\n",
    "                                    #print(\"debug:POINT\")\n",
    "                                    k=0\n",
    "                                    k+=1\n",
    "                                else:\n",
    "                                    k=0\n",
    "                                    k+=1\n",
    "\n",
    "\n",
    "    ac.close()\n",
    "    allc.close()\n",
    "    print(\"basal contacts saved allpts=\",allpts,\"deci_pts=\",deci_points)\n",
    "    print(\"saved as\",path_in+'all_contacts.csv',\"and\",path_in+'contacts.csv')\n",
    "    return(ls_dict,ls_dict_decimate)\n",
    "\n",
    "save_basal_contact_polarity(path_in,dtm,geol_clip,contact_decimate,c_l,intrusion_mode,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a Analyse fault-fault topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T03:16:02.206127Z",
     "start_time": "2020-03-25T03:15:59.170150Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_topology.parse_fault_relationships(graph_path,tmp_path,output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# loop2LoopStructural test \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T03:23:46.110061Z",
     "start_time": "2020-03-25T03:16:02.213105Z"
    }
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "import lavavu\n",
    "from pyamg import solve\n",
    "\n",
    "m2l_export.loop2LoopStructural(output_path+'formation_thicknesses.csv',output_path+'orientations.csv',output_path+'contacts4.csv',bbox)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T03:23:46.126017Z",
     "start_time": "2020-03-25T03:23:46.116044Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"m2l\",(t1-t0)/60.0,\"LoopStructural\",(t2-t1)/60.0,\"Total\",(t2-t0)/60.0,\"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:45:53.982705Z",
     "start_time": "2020-03-18T03:45:47.043626Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
