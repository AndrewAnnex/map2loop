{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map2loop: From geology layers to outputs to various 3D modelling programs (geomodeller/loopstructural/gempy/noddy)- SMG example \n",
    "\n",
    "This notebook reads in three layers from  local or remote sources:  geology polygons, orientation data and fault polylines; and calculates the topological relationships between the different features. Requires compiled cpp code from Vitaliy Ogarko\n",
    "\n",
    "This all gets fed into successive tolopogical and geometric transfroms that end up feeding into a mdelling engine to make a 3D model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:31:45.485487Z",
     "start_time": "2020-05-21T00:31:42.334356Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import stat\n",
    "import functools \n",
    "import operator  \n",
    "import matplotlib\n",
    "import networkx as nx\n",
    "import rasterio\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "from map2loop import m2l_utils\n",
    "from map2loop import m2l_topology\n",
    "from map2loop import m2l_geometry\n",
    "from map2loop import m2l_interpolation\n",
    "from map2loop import m2l_export\n",
    "import time\n",
    "import shutil\n",
    "%matplotlib inline\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "if(os.path.exists('C:\\\\Users\\\\00073294')):\n",
    "    #newwd=\"C:\\\\Users\\\\00073294\\Dropbox\\\\loop_minex\\\\map2model\\\\\"\n",
    "    #os.chdir(newwd)\n",
    "    print(\"Current Working Directory \" )\n",
    "\n",
    "    gdal_data = os.environ['GDAL_DATA']\n",
    "    print(\"***\",gdal_data)\n",
    "    print('is dir: ' + str(os.path.isdir(gdal_data)))\n",
    "    gcs_csv = os.path.join(gdal_data, 'gcs.csv')\n",
    "    print('is file: ' + str(os.path.isfile(gcs_csv)))\n",
    "    st = os.stat(gcs_csv)\n",
    "    print('is readable: ' + str(bool(st.st_mode & stat.S_IRGRP)))\n",
    "    os.environ['PROJ_LIB']=r\"C:\\\\Users\\\\00073294\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\Lib\\\\site-packages\\\\pyproj\\\\proj_dir\\\\share\\\\proj\"\n",
    "    print(os.getenv('PROJ_LIB'))\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bounding box based on inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:31:45.503458Z",
     "start_time": "2020-05-21T00:31:45.487458Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data_name='test_data3'\n",
    "\n",
    "test_data_path='../'+test_data_name+'/'\n",
    "\n",
    "os.chdir(test_data_path)\n",
    "%run -i \"m2l_config.py\"\n",
    "#%run -i \"m2l_config_remote.py\"\n",
    "print(os.getcwd())\n",
    "\n",
    "bbox2=str(minx)+\",\"+str(miny)+\",\"+str(maxx)+\",\"+str(maxy)\n",
    "lat_point_list = [miny, miny, maxy, maxy, maxy]\n",
    "lon_point_list = [minx, maxx, maxx, minx, minx]\n",
    "bbox_geom = Polygon(zip(lon_point_list, lat_point_list))\n",
    "polygon = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "bbox=(minx,miny,maxx,maxy)\n",
    "\n",
    "#if(os.path.isdir('./data')):\n",
    "#    shutil.rmtree('./data', ignore_errors=False) \n",
    "#if(not os.path.isdir('./data')):\n",
    "#    os.mkdir('./data')\n",
    "#shutil.copytree(source_data_path,'./data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:31:45.520462Z",
     "start_time": "2020-05-21T00:31:45.506459Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fold_decimate=2         \n",
    "\n",
    "fault_decimate=0\n",
    "\n",
    "contact_decimate=5\n",
    "\n",
    "orientation_decimate=0\n",
    "\n",
    "use_interpolations=True       #use interpolated dips/contacts as additional constraints\n",
    "\n",
    "use_fat=True                   #use fold axial trace orientation hints\n",
    "\n",
    "pluton_form='domes'\n",
    "\n",
    "fault_dip=90\n",
    "\n",
    "min_fault_length=5000\n",
    "\n",
    "compute_etc=False\n",
    "\n",
    "\n",
    "\n",
    "#local_paths=False\n",
    "\n",
    "#################################\n",
    "# There are many alternative datasets that \n",
    "# can be extracted from the input data, \n",
    "# and many choices of possible input data\n",
    "#\n",
    "# These flags define what the actual workflow \n",
    "# will be for this experiment, based partly \n",
    "# on which (if any) modelling engine is used\n",
    "#\n",
    "#\n",
    "#    choices:         \\model engine/\n",
    "#    loopstructural    \\          /\n",
    "#    geomodeller        \\        /\n",
    "#    gempy               \\      /\n",
    "#    noddy                \\    /\n",
    "#    null                  \\  /\n",
    "#                           \\/\n",
    "#############################################\n",
    "workflow={'model_engine':'geomodeller'} \n",
    "\n",
    "if(workflow['model_engine']=='geomodeller'):\n",
    "    workflow.update( {'seismic_section':False,\n",
    "          'cover_map':False,\n",
    "          'near_fault_interpolations':True,\n",
    "          'fold_axial_traces':False,\n",
    "          'stereonets':True,\n",
    "          'formation_thickness':False,\n",
    "          'polarity':False,\n",
    "          'strat_offset':False,\n",
    "          'contact_dips':True} )\n",
    "elif(workflow['model_engine']=='loopstructural'):\n",
    "    workflow.update( {'seismic_section':False,\n",
    "          'cover_map':False,\n",
    "          'near_fault_interpolations':False,\n",
    "          'fold_axial_traces':True,\n",
    "          'stereonets':True,\n",
    "          'formation_thickness':True,\n",
    "          'polarity':False,\n",
    "          'strat_offset':False,\n",
    "          'contact_dips':False} )\n",
    "elif(workflow['model_engine']=='gempy'):\n",
    "    workflow.update( {'seismic_section':False,\n",
    "          'cover_map':False,\n",
    "          'near_fault_interpolations':False,\n",
    "          'fold_axial_traces':True,\n",
    "          'stereonets':False,\n",
    "          'formation_thickness':False,\n",
    "          'polarity':False,\n",
    "          'strat_offset':False,\n",
    "          'contact_dips':False} )\n",
    "elif(workflow['model_engine']=='noddy'):\n",
    "    workflow.update( {'seismic_section':False,\n",
    "          'cover_map':False,\n",
    "          'near_fault_interpolations':False,\n",
    "          'fold_axial_traces':False,\n",
    "          'stereonets':False,\n",
    "          'formation_thickness':False,\n",
    "          'polarity':False,\n",
    "          'strat_offset':False,\n",
    "          'contact_dips':False} )\n",
    "else:\n",
    "    workflow.update( {'seismic_section':False,\n",
    "          'cover_map':False,\n",
    "          'near_fault_interpolations':False,\n",
    "          'fold_axial_traces':False,\n",
    "          'stereonets':True,\n",
    "          'formation_thickness':True,\n",
    "          'polarity':False,\n",
    "          'strat_offset':True,\n",
    "          'contact_dips':False} )\n",
    "    \n",
    "\n",
    "\n",
    "# no cover info so no need load cover layers\n",
    "if(not workflow['cover_map']):\n",
    "    dtb=0\n",
    "    dtb_null=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we test to see if we have access to the online data we need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:31:46.740745Z",
     "start_time": "2020-05-21T00:31:45.524462Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "loopwfs=m2l_utils.have_access(\"geo.loop-gis.org\")\n",
    "ga=m2l_utils.have_access(\"services.ga.gov.au\")\n",
    "\n",
    "if(not (loopwfs & ga)):\n",
    "    local_paths=True\n",
    "    net=False\n",
    "    print('using local paths')\n",
    "else:\n",
    "    net=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot geology polygons and bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:31:47.529896Z",
     "start_time": "2020-05-21T00:31:46.742712Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(geology_file)\n",
    "geology_ll = gpd.read_file(geology_file,bbox=bbox)\n",
    "\n",
    "geology_ll[c_l['g']].fillna(geology_ll[c_l['g2']], inplace=True)\n",
    "geology_ll[c_l['g']].fillna(geology_ll[c_l['c']], inplace=True)\n",
    "display(geology_ll.head())\n",
    "base=geology_ll.plot(column=c_l['c'],figsize=(10,10),edgecolor='#000000',linewidth=0.2)\n",
    "polygon.plot(ax=base, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save geology to file as WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:31:48.174918Z",
     "start_time": "2020-05-21T00:31:47.531896Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "hint_flag=False # use GSWA strat database to provide relative age hints\n",
    "sub_geol = geology_ll[['geometry', c_l['o'],c_l['c'],c_l['g'],c_l['u'],c_l['min'],c_l['max'],c_l['ds'],c_l['r1'],c_l['r2']]]\n",
    "m2l_topology.save_geol_wkt(sub_geol,geology_file_csv, c_l,hint_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save mineral deposits to file as WKT\n",
    "This is not needed by map2loop to build 3D models, but is used by map2model to calculate mineral deposit/topology analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:31:48.937953Z",
     "start_time": "2020-05-21T00:31:48.177918Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mindep = gpd.read_file(mindep_file,bbox=bbox)\n",
    "\n",
    "sub_mindep = mindep[['geometry', c_l['msc'],c_l['msn'],c_l['mst'],c_l['mtc'],c_l['mscm'],c_l['mcom']]]\n",
    "m2l_topology.save_mindep_wkt(sub_mindep,mindep_file_csv, c_l)\n",
    "\n",
    "base=sub_mindep.plot()\n",
    "polygon.plot(ax=base, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and save WAROX point data as WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:31:49.454018Z",
     "start_time": "2020-05-21T00:31:48.941947Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "warox = gpd.read_file(structure_file,bbox=bbox)\n",
    "\n",
    "sub_pts = warox[['geometry', c_l['gi'],c_l['d'],c_l['dd']]]\n",
    "\n",
    "m2l_topology.save_structure_wkt(sub_pts,structure_file_csv,c_l)\n",
    "\n",
    "base=sub_pts.plot()\n",
    "polygon.plot(ax=base, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot faults and bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:31:49.783016Z",
     "start_time": "2020-05-21T00:31:49.457018Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "lines_ll=gpd.read_file(fault_file,bbox=bbox)\n",
    "\n",
    "base2=lines_ll.plot(cmap='rainbow',column=c_l['f'],figsize=(10,10),linewidth=0.4)\n",
    "polygon.plot(ax=base2, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save faults to file as WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:31:49.901017Z",
     "start_time": "2020-05-21T00:31:49.785019Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sub_lines = lines_ll[['geometry', c_l['o'],c_l['f']]]\n",
    "m2l_topology.save_faults_wkt(sub_lines,fault_file_csv,c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create map2model input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:31:49.909024Z",
     "start_time": "2020-05-21T00:31:49.903017Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "m2l_topology.save_Parfile(m2m_cpp_path,c_l,graph_path,geology_file_csv,fault_file_csv,structure_file_csv,mindep_file_csv,minx,maxx,miny,maxy,500.0,'Fe,Cu,Au,NONE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:21.027471Z",
     "start_time": "2020-05-21T00:31:49.911016Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import platform\n",
    "os.chdir(m2m_cpp_path)\n",
    "print(os.getcwd())\n",
    "#%system map2model.exe Parfile\n",
    "if(platform.system()=='Windows'):\n",
    "    subprocess.run([\"map2model.exe\", \"Parfile\"], capture_output=True)\n",
    "else:\n",
    "    subprocess.run([\"./map2model\", \"Parfile\"], capture_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple network graph of the geology with legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:21.708498Z",
     "start_time": "2020-05-21T00:32:21.030472Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "G=nx.read_gml(strat_graph_file,label='id')\n",
    "selected_nodes = [n for n,v in G.nodes(data=True) if n >=0]\n",
    "nx.draw_networkx(G, pos=nx.kamada_kawai_layout(G), arrows=True, nodelist=selected_nodes)\n",
    "\n",
    "nlist=list(G.nodes.data('LabelGraphics'))\n",
    "nlist.sort()\n",
    "for no in nlist:\n",
    "    if(no[0]>=0):\n",
    "        elem=str(no[1]).replace(\"{'text':\",\"\").replace(\", 'fontSize': 14}\",\"\")\n",
    "        #second=elem.split(\":\").replace(\"'\",\"\")\n",
    "        print(no[0],\" \",elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Process topography, stratigraphy, fold axial traces and faults\n",
    "\n",
    "### Takes GML file produced by topology code, combines with geology polygons, structure points and dtm to create 3D model in gempy.<br><br>\n",
    "\n",
    "Limitations:  no dykes, no sills. Sills require us to assign a unique surface to each instance of a sill (sill between units A and B needs to be different from sill of same age and strat codes as one found between E and F). Dykes via cokriging are really hard without just cookie cutting them in (but that is not our problem!). We are not checking for onlap relationships, which can perhaps been seen by having lots of units from one series adjacent to the youngest surface of the older series. Could also think about interpreting these as faults to introduce conceptual uncertainty. All mistakes belong to Mark Jessell, topology code that feeds this system by Vitaliy Ogarko.<br><br>\n",
    "\n",
    "Geology layer needs to have some unique strat code or text, some group code or text to function<br>\n",
    "Structure layer needs dip/dip direction<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:21.716477Z",
     "start_time": "2020-05-21T00:32:21.710480Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('../map2loop')\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "sys.path.insert(0,\"../..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "#print(os.getcwd())\n",
    "#os.environ[\"PROJ_LIB\"] = r\"C:\\Users\\00073294\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\pyproj\\proj_dir\\share\\proj\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we define an area of interest and some other basic stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:21.730478Z",
     "start_time": "2020-05-21T00:32:21.719476Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "#test_data_path='../test_data3/'\n",
    "\n",
    "\n",
    "#%run -i \"../test_data3/m2l_config.py\"\n",
    "\n",
    "bbox2=str(minx)+\",\"+str(miny)+\",\"+str(maxx)+\",\"+str(maxy)\n",
    "lat_point_list = [miny, miny, maxy, maxy, maxy]\n",
    "lon_point_list = [minx, maxx, maxx, minx, minx]\n",
    "bbox_geom = Polygon(zip(lon_point_list, lat_point_list))\n",
    "polygon = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "bbox=(minx,miny,maxx,maxy)\n",
    "\n",
    "step_out=0.045 #add (in degrees) so edge pixel from dtm reprojection are not found\n",
    "\n",
    "\n",
    "#contact_decimate=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Download and reproject the appropriate SRTM data\n",
    "mj: Getting this from GA, but could also get from Hawaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:22.103476Z",
     "start_time": "2020-05-21T00:32:21.733478Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "polygon_ll=polygon.to_crs(src_crs)\n",
    "\n",
    "minlong=polygon_ll.total_bounds[0]-step_out\n",
    "maxlong=polygon_ll.total_bounds[2]+step_out\n",
    "minlat=polygon_ll.total_bounds[1]-step_out\n",
    "maxlat=polygon_ll.total_bounds[3]+step_out\n",
    "\n",
    "print(minlong,maxlong,minlat,maxlat)\n",
    "if(((not os.path.exists(dtm_file)) or (not local_paths)) and net):    \n",
    "    m2l_utils.get_dtm(dtm_file, minlong,maxlong,minlat,maxlat)\n",
    "    geom_rp=m2l_utils.reproject_dtm(dtm_file,dtm_reproj_file,src_crs,dst_crs)\n",
    "\n",
    "dtm = rasterio.open(dtm_reproj_file)\n",
    "pyplot.imshow(dtm.read(1), cmap='terrain',vmin=0,vmax=1000)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Load stratigraphy graph and create list of series (aka groups)\n",
    "mj: The choice of what constitutes basic unit and what a group of units is hard-wired at the moment, but could be altered to any pair. Not even sure we need two levels but it seemed like a good idea at the time. Note that this needs the arcgis plugin version of the topology code (for now) as it seperates the different sub graphs. Text outputs list alternate topologies for series and surfaces, which if confirmed by comapring max-min ages will be a nice source of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:24.419561Z",
     "start_time": "2020-05-21T00:32:22.106478Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groups,glabels,G = m2l_topology.get_series(strat_graph_file,'id')\n",
    "m2l_topology.save_units(G,tmp_path,glabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Load geology & structure data\n",
    "Currently loading from local files, but could load geology from WFS server at GSWA EXCEPT that the WFS online map has less fields that the zipped shapefiles. Go figure. We don't use fault layer at the moment (except for Vitaliy's topology code) but same logic applies in terms of where to get it from. Already have fault/strat relationships and once we have fault/fault relationships will start to include faults in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:25.084527Z",
     "start_time": "2020-05-21T00:32:24.421524Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract point data from structure & geology layers for modelling\n",
    "##First we readin the structure and map from shapefiles, or wherever...\n",
    "\n",
    "bbox=(minx,miny,maxx,maxy)\n",
    "geology = gpd.read_file(geology_file,bbox=bbox)\n",
    "geology[c_l['g']].fillna(geology[c_l['g2']], inplace=True)\n",
    "geology[c_l['g']].fillna(geology[c_l['c']], inplace=True)\n",
    "\n",
    "\n",
    "structure = gpd.read_file(structure_file,bbox=bbox)\n",
    "structure.crs=dst_crs\n",
    "print(fault_file)\n",
    "faults = gpd.read_file(fault_file,bbox=bbox)\n",
    "faults.crs=dst_crs\n",
    "#display(faults)\n",
    "sub_pts = structure[['geometry',c_l['d'],c_l['dd'],c_l['sf']]] \n",
    "\n",
    "base=geology.plot(column=c_l['c'],figsize=(10,10),edgecolor='#000000',linewidth=0.2)\n",
    "sub_pts.plot(ax=base,edgecolor='black')\n",
    "faults.plot(ax=base, column=c_l['f'],edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Clip geology, faults, structures and map geology to structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:28.015588Z",
     "start_time": "2020-05-21T00:32:25.087527Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "geology = m2l_utils.explode(geology)\n",
    "geology.crs = dst_crs\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "structure_code = gpd.sjoin(sub_pts, geology, how=\"left\", op=\"within\")\n",
    "\n",
    "y_point_list = [miny, miny, maxy, maxy, miny]\n",
    "x_point_list = [minx, maxx, maxx, minx, minx]\n",
    "\n",
    "bbox_geom = Polygon(zip(x_point_list, y_point_list))\n",
    "\n",
    "polygo = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "#display(polygo.geometry)\n",
    "is_bed=structure_code[c_l['sf']].str.contains(c_l['bedding'], regex=False) \n",
    "    \n",
    "all_beds = structure_code[is_bed]\n",
    "#display(sfcode)\n",
    "if(workflow['fold_axial_traces']):\n",
    "    all_folds=faults[faults[c_l['f']].str.contains(c_l['fold'])]\n",
    "    folds_clip=m2l_utils.clip_shp(all_folds,polygo)\n",
    "    folds_clip.to_file(tmp_path+'folds_clip.shp')\n",
    "\n",
    "\n",
    "all_faults=faults[faults[c_l['f']].str.contains(c_l['fault'])]\n",
    "\n",
    "\n",
    "\n",
    "#display(structure_code)\n",
    "#geol_clip=m2l_utils.clip_shp(geology, polygo)\n",
    "geol_clip=gpd.overlay(geology, polygo, how='intersection')\n",
    "faults_clip=m2l_utils.clip_shp(all_faults,polygo)\n",
    "#display(faults_clip)\n",
    "structure_clip = m2l_utils.clip_shp(all_beds, polygo)\n",
    "#display(structure_clip)\n",
    "base = geol_clip.plot(column=c_l['c'],figsize=(7,7),edgecolor='#000000',linewidth=0.2)\n",
    "faults_clip.plot(ax=base, column=c_l['f'],edgecolor='black')\n",
    "structure_clip.plot(ax=base, column=c_l['c'],edgecolor='black')\n",
    "\n",
    "\n",
    "if(c_l['dd']=='strike'):\n",
    "    structure_clip['azimuth2'] = structure_clip.apply(lambda row: row[c_l['dd']]+90.0, axis = 1)\n",
    "    c_l['dd']='azimuth2'\n",
    "    \n",
    "    \n",
    "geol_clip.to_file(tmp_path+'geol_clip.shp')\n",
    "faults_clip.to_file(tmp_path+'faults_clip.shp')\n",
    "structure_clip.to_file(tmp_path+'structure_clip.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create possible stratigraphy sets per group\n",
    "mj: <font color='red'>Uses first of each possible set of toplogies per unit and per group, which is arbitrary. </font>On the other hand we are not checking relative ages again to see if this helps reduce ambiguity, which I think it would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:28.549628Z",
     "start_time": "2020-05-21T00:32:28.019589Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2l_topology.save_group(G,tmp_path,glabels,geol_clip,c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Export orientation data in csv  format\n",
    "mj: Orientation data needs calculated height as file does not provide it, taken from SRTM data already downloaded. To calculate polarity <font color='red'>(WHICH WE DON'T DO YET)</font> we can calculate the dot product of the dip direction of a bedding plane and the vector to that points nearest basal contact node, if  abs(acos(dot product))>90  then right way up?\n",
    "\n",
    "\n",
    "\n",
    "Added code to not save intrusion orientation data as they won't have associated surfaces if sill..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:29.204597Z",
     "start_time": "2020-05-21T00:32:28.551597Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "m2l_geometry.save_orientations(structure_clip,output_path,c_l,orientation_decimate,dtm,dtb,dtb_null,workflow['cover_map'])\n",
    "\n",
    "m2l_utils.plot_points(output_path+'orientations.csv',geol_clip, 'formation','X','Y',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Display stereonets of bedding by formations and group to see how we can combine them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:42.762038Z",
     "start_time": "2020-05-21T00:32:29.207595Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(workflow['stereonets']):\n",
    "    orientations=pd.read_csv(output_path+'orientations.csv',\",\")\n",
    "    all_sorts=pd.read_csv(tmp_path+'all_sorts.csv',\",\")\n",
    "            \n",
    "    m2l_utils.plot_bedding_stereonets(orientations,all_sorts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Find those series that don't have any orientation or contact point data  then create arbitrary point for series with no orientation data\n",
    "Not sure if gempy needs this but geomodeller does. Currently just gives a point dipping 45 degrees to North, but could use dip direction normal to basal surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:43.073128Z",
     "start_time": "2020-05-21T00:32:42.767037Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2l_geometry.create_orientations( tmp_path, output_path, dtm,dtb,dtb_null,workflow['cover_map'],geol_clip,structure_clip,c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Export contact information subset of each polygon to gempy format\n",
    "mj: Orientation data needs calculated height as file does not provide it, taken from SRTM data already downloaded. Need to reduce number of points whilst retaining useful info (Ranee's job!)'\n",
    "To calculate which are the basal units contact for a polygon find the polygons which are older than the selected polygon, in the example below the central polygon has relative age 23 so its basal contact is with the polygons whose ages are 26 & 28. If there are no older units for a polygon it has no basal content. We keep every nth node based on the decimate term (simple count along polyline). gempy seems to need at least two points per surface, so we always take the first two points.\n",
    "\n",
    "\n",
    "<img src='../graphics/base.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:51.840388Z",
     "start_time": "2020-05-21T00:32:43.076128Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls_dict,ls_dict_decimate=m2l_geometry.save_basal_contacts(tmp_path,dtm,dtb,dtb_null,workflow['cover_map'],geol_clip,contact_decimate,c_l,intrusion_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all basal contacts that are defined by faults and save to shapefile (no decimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:53.780643Z",
     "start_time": "2020-05-21T00:32:51.843385Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "m2l_geometry.save_basal_no_faults(tmp_path+'basal_contacts.shp',tmp_path+'faults_clip.shp',ls_dict,10,c_l,dst_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove faults from decimated basal contacts as save as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:32:56.342747Z",
     "start_time": "2020-05-21T00:32:53.783643Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "contacts=gpd.read_file(tmp_path+'basal_contacts.shp')\n",
    "\n",
    "m2l_geometry.save_basal_contacts_csv(contacts,output_path,dtm,dtb,dtb_null,workflow['cover_map'],contact_decimate,c_l)\n",
    "\n",
    "m2l_utils.plot_points(output_path+'contacts4.csv',geol_clip, 'formation','X','Y',False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New interpolation test\n",
    "Interpolates a regular grid of orientations from an  shapefile of arbitrarily-located points and saves out four csv files of l,m & n direction cosines and dip dip direction data\n",
    "\n",
    "Can choose between various RBF and IDW options   \n",
    "  \n",
    "The purpose of these interpolations and associated code is to help in three cases:\n",
    "- Providing estimated dips and contacts in fault-bounded domains where no structural data are available\n",
    "- Needed to estimate true thickness of formations\n",
    "- Possibly useful for populating parts of maps where little structural data is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:33:08.939788Z",
     "start_time": "2020-05-21T00:32:56.346704Z"
    }
   },
   "outputs": [],
   "source": [
    "super_groups=(('Wyloo Group','Shingle Creek Group'),('Turee Creek Group','Hamersley Group','Fortescue Group'),('A-mgn-PRK', 'A-b-PRK', 'A-mgn-PMI','A-mgn-PRK', 'A-s-PMI', 'A-s-PRK') ) \n",
    "\n",
    "basal_contacts=tmp_path+'basal_contacts.shp'\n",
    "spacing=1000\n",
    "orientation_interp,contact_interp,combo_interp=m2l_interpolation.interpolation_grids(geology_file,structure_file,basal_contacts,bbox,spacing,dst_crs,scheme,super_groups,c_l)\n",
    "\n",
    "\n",
    "f=open(tmp_path+'interpolated_orientations.csv','w')\n",
    "f.write('X,Y,l,m,n,dip,dip_dir\\n')\n",
    "for row in orientation_interp:\n",
    "    ostr='{},{},{},{},{},{},{}\\n'.format(row[0],row[1],row[2],row[3],row[4],row[5],row[6])\n",
    "    f.write(ostr)\n",
    "f.close()\n",
    "f=open(tmp_path+'interpolated_contacts.csv','w')\n",
    "f.write('X,Y,l,m,angle\\n')\n",
    "for row in contact_interp:\n",
    "    ostr='{},{},{},{},{}\\n'.format(row[0],row[1],row[2],row[3],row[4])\n",
    "    f.write(ostr)\n",
    "f.close()\n",
    "f=open(tmp_path+'interpolated_combined.csv','w')\n",
    "f.write('X,Y,l,m,n,dip,dip_dir\\n')\n",
    "for row in combo_interp:\n",
    "    ostr='{},{},{},{},{},{},{}\\n'.format(row[0],row[1],row[2],row[3],row[4],row[5],row[6])\n",
    "    f.write(ostr)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:33:09.147789Z",
     "start_time": "2020-05-21T00:33:08.941782Z"
    }
   },
   "outputs": [],
   "source": [
    "x=int((bbox[2]-bbox[0])/spacing)+1\n",
    "y=int((bbox[3]-bbox[1])/spacing)+1\n",
    "\n",
    "dip_grid=np.ones((y,x))\n",
    "dip_grid=dip_grid*-999\n",
    "dip_dir_grid=np.ones((y,x))\n",
    "dip_dir_grid=dip_dir_grid*-999\n",
    "for row in combo_interp:\n",
    "    r=int((row[1]-bbox[1])/spacing)\n",
    "    c=int((row[0]-bbox[0])/spacing)\n",
    "    dip_grid[r,c]=float(row[5])\n",
    "    dip_dir_grid[r,c]=float(row[6])\n",
    "\n",
    "print('interpolated dips')\n",
    "plt.imshow(dip_grid, cmap=\"hsv\",origin='lower')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:33:09.315307Z",
     "start_time": "2020-05-21T00:33:09.150792Z"
    }
   },
   "outputs": [],
   "source": [
    "print('interpolated dip directions')\n",
    "       \n",
    "plt.imshow(dip_dir_grid, cmap=\"hsv\",origin='lower')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process fault geometry\n",
    "Save Faults as decimated points and representative orientation  \n",
    "Then, for each  fault string:\n",
    "- incementally advance along polyline every at each inter-node (no point in doing more?)\n",
    "- find local stratigraphy 10m to left and right of fault\n",
    "  \n",
    "Once full fault has been traversed:\n",
    "- Find list of contacts left \n",
    "- Find equivalent contacts on right\n",
    "- use interpolated orientations to estimate minimum true offset assuming vertical displacement and store \n",
    "- if no equivalent found, flag as domain fault and find min strat offset for contact, use cumulative minimum thickness estimate and store with flag (not implemented)\n",
    "- estimate median & sd of minimum fault offset and store with flag (not implemented)\n",
    "\n",
    "Local Orientations\n",
    "Since much of the code is the same, we benefit by calculating local orientation data either side of fault so that geomodeller/gempy have satisfied fault compartment orientation data## Save fault as contact info and and orientation info make vertical (for the moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:33:09.813340Z",
     "start_time": "2020-05-21T00:33:09.317307Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "m2l_geometry.save_faults(tmp_path+'faults_clip.shp',output_path,dtm,dtb,dtb_null,workflow['cover_map'],c_l,fault_decimate,min_fault_length,fault_dip)\n",
    "\n",
    "use_gcode=('Proterozoic mafic intrusive unit','Edmund Group','Moorarie Supersuite','Wyloo Group','Wooly Dolomite','Shingle Creek Group','Balgara Dolerite','Munder Formation','Anthiby Formation','Turee Creek Group','Weeli Wolli Formation','Hamersley Group','Fortescue Group','Metawandy Granite','Wyloo Inlier greenstones' ) \n",
    "use_gcode2=('Proterozoic_mafic_intrusive_unit','Edmund_Group','Moorarie_Supersuite','Wyloo_Group','Wooly_Dolomite','Shingle_Creek_Group','Balgara_Dolerite','Munder_Formation','Anthiby_Formation','Turee_Creek_Group','Weeli_Wolli_Formation','Hamersley_Group','Fortescue_Group','Metawandy_Granite','Wyloo_Inlier_greenstones' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:34:34.496599Z",
     "start_time": "2020-05-21T00:33:09.815336Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2l_interpolation.process_fault_throw_and_near_faults_from_grid(tmp_path,output_path,dtm_reproj_file,dtb,dtb_null,workflow['cover_map'],c_l,use_gcode,use_gcode2,dst_crs,bbox,\n",
    "                                                                scheme,dip_grid,dip_dir_grid,x,y,spacing)\n",
    "\n",
    "m2l_utils.plot_points(output_path+'fault_displacements3.csv',geol_clip, 'apparent_displacement','X','Y',False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process plutons\n",
    "\n",
    "For each instruve but not sill polygon, find older neighbours and store decimated contact points. Also store dipping contact orientations (user defined, just because) with four possible sub-surface configurations:\n",
    "\n",
    "<b>saucers: \\\\_+++_/ <br>\n",
    "batholiths: +++/__ __ _\\\\+++  <br> \n",
    "domes: /‾+++‾\\\\ <br>\n",
    "pendants: +++\\\\_  _/+++ <br>\n",
    "</b>\n",
    "  \n",
    "Saves out orientations and contact points, as well as updated group level stratigraphic column.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:34:34.931605Z",
     "start_time": "2020-05-21T00:34:34.498601Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "bbox=(minx,miny,maxx,maxy)\n",
    "\n",
    "pluton_dip=str(pluton_dip)\n",
    "\n",
    "dist_buffer=10\n",
    "\n",
    "m2l_geometry.process_plutons(tmp_path,output_path,geol_clip,local_paths,dtm,dtb,dtb_null,workflow['cover_map'],pluton_form,pluton_dip,contact_decimate,c_l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract faults and basal contacts of groups from seismic section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:34:34.945606Z",
     "start_time": "2020-05-21T00:34:34.933604Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(workflow['seismic_section']):\n",
    "    seismic_line_file=data_path+'seismic_line_10GA-CP1_rev.shp'   #input geology file (if local)\n",
    "    seismic_line = gpd.read_file(seismic_line_file) #import map\n",
    "    seismic_line.plot(figsize=(10,10),edgecolor='#000000',linewidth=0.2) #display map\n",
    "    display(seismic_line)\n",
    "\n",
    "\n",
    "    seismic_bbox_file=data_path+'seismic_bbox.shp'   #input geology file (if local)\n",
    "    seismic_bbox = gpd.read_file(seismic_bbox_file) #import map\n",
    "    seismic_bbox.set_index('POSITION',inplace=True)\n",
    "\n",
    "    seismic_interp_file=data_path+'seismic_interp.shp'   #input geology file (if local)\n",
    "    seismic_interp = gpd.read_file(seismic_interp_file) #import map\n",
    "    seismic_interp.plot(column='FEATURE',figsize=(10,10),edgecolor='#000000',linewidth=0.5) #display map\n",
    "    display(seismic_interp)\n",
    "\n",
    "    surface_cut=2000\n",
    "\n",
    "    m2l_geometry.extract_section(tmp_path,output_path,seismic_line,seismic_bbox,seismic_interp,dtm,dtb,dtb_null,workflow['cover_map'],surface_cut)\n",
    "\n",
    "    contacts=pd.read_csv(output_path+'contacts4.csv',\",\")\n",
    "    seismic_contacts=pd.read_csv(output_path+'seismic_base.csv',\",\")\n",
    "    all_contacts=pd.concat([contacts,seismic_contacts],sort=False)\n",
    "    all_contacts.to_csv (output_path+'contacts4.csv', index = None, header=True)\n",
    "\n",
    "    faults=pd.read_csv(output_path+'faults.csv',\",\")\n",
    "    seismic_faults=pd.read_csv(output_path+'seismic_faults.csv',\",\")\n",
    "    all_faults=pd.concat([faults,seismic_faults],sort=False)\n",
    "    all_faults.to_csv (output_path+'faults.csv', index = None, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagate dips along contacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:34:58.817792Z",
     "start_time": "2020-05-21T00:34:34.947635Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['contact_dips']):\n",
    "    orientations=pd.read_csv(output_path+'orientations.csv',\",\")\n",
    "    contact_dip=-999\n",
    "    contact_orientation_decimate=5\n",
    "    m2l_geometry.save_basal_contacts_orientations_csv(contacts,orientations,geol_clip,tmp_path,output_path,dtm,dtb,\n",
    "                            dtb_null,workflow['cover_map'],contact_orientation_decimate,c_l,contact_dip,dip_grid,spacing,bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate formation thickness and normalised formation thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:34:58.825788Z",
     "start_time": "2020-05-21T00:34:58.819787Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['formation_thickness']):\n",
    "    buffer =5000\n",
    "    max_thickness_allowed=10000\n",
    "\n",
    "    #m2l_geometry.calc_thickness(tmp_path,output_path,buffer,max_thickness_allowed,c_l)\n",
    "    m2l_geometry.calc_thickness_with_grid(tmp_path,output_path,buffer,max_thickness_allowed,\n",
    "                                          c_l,bbox,dip_grid,dip_dir_grid,x,y,spacing)\n",
    "    m2l_geometry.normalise_thickness(output_path)\n",
    "    \n",
    "    m2l_utils.plot_points(output_path+'formation_thicknesses_norm.csv',geol_clip,'norm_th','x','y',False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates fold axial trace points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:34:58.835789Z",
     "start_time": "2020-05-21T00:34:58.828789Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['fold_axial_traces']):\n",
    "\n",
    "    m2l_geometry.save_fold_axial_traces(tmp_path+'folds_clip.shp',output_path,dtm,dtb,dtb_null,workflow['cover_map'],c_l,fold_decimate)\n",
    "\n",
    "    #Save fold axial trace near-hinge orientations\n",
    "    fat_step=750         # how much to step out normal to fold axial trace\n",
    "    close_dip=20.0       #dip to assign to all new orientations\n",
    "\n",
    "    m2l_geometry.save_fold_axial_traces_orientations(tmp_path+'folds_clip.shp',output_path,tmp_path,dtm,dtb,dtb_null,workflow['cover_map'],c_l,dst_crs,fold_decimate,fat_step,close_dip,scheme)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data to ensure it meets modelling requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:37:35.668129Z",
     "start_time": "2020-05-21T00:37:35.660158Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "use_gcode=''\n",
    "if(local_paths): ###############FUDGE#############\n",
    "    if(workflow['model_engine'] =='geomodeller' ):\n",
    "        use_gcode=('Hamersley_Group','Fortescue_Group','A_mgn_PRK',  'A_mgn_PMI' ) ################# MOVE UP   #########################\n",
    "    elif(workflow['model_engine']=='loopstructural'):\n",
    "        use_gcode=('Hamersley_Group','Fortescue_Group','Turee_Creek_Group','A_mgn_PRK',  'A_mgn_PMI' ) ################# MOVE UP   #########################\n",
    "    elif(workflow['model_engine']=='gempy'):\n",
    "        use_gcode=('Hamersley_Group','Fortescue_Group','Wyloo_Group','Shingle_Creek_Group','Turee_Creek_Group','A_mgn_PRK',  'A_mgn_PMI' ) ################# MOVE UP   #########################\n",
    "    elif(workflow['model_engine']=='noddy'):\n",
    "        use_gcode=('')\n",
    "else:\n",
    "    use_gcode=('Hamersley_Group','Fortescue_Group','Wyloo_Group','Shingle_Creek_Group','Turee_Creek_Group','A_mgn_PMI',  'A_mgn_PRK' ) ################# MOVE UP   #########################\n",
    "print('only processing',use_gcode)\n",
    "\n",
    "inputs=('')\n",
    "\n",
    "if(workflow['model_engine'] =='geomodeller'):\n",
    "    inputs=('invented_orientations','intrusive_orientations','fat_orientations','near_fault_orientations','fault_tip_contacts','contact_orientations')\n",
    "elif(workflow['model_engine']=='loopstructural'):\n",
    "    inputs=('invented_orientations','fat_orientations')\n",
    "elif(workflow['model_engine']=='gempy'):\n",
    "    inputs=('invented_orientations','interpolated_orientations','fat_orientations')\n",
    "elif(workflow['model_engine']=='noddy'):\n",
    "    inputs=('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:37:48.094980Z",
     "start_time": "2020-05-21T00:37:36.866130Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m2l_geometry.tidy_data(output_path,tmp_path,use_gcode,use_interpolations,use_fat,pluton_form,inputs,workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate polarity of original bedding orientation data (not used yet in final calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:37:48.100984Z",
     "start_time": "2020-05-21T00:37:48.096981Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['polarity']):\n",
    "    m2l_geometry.save_orientations_with_polarity(output_path+'orientations.csv',output_path,c_l,tmp_path+'basal_contacts.shp',tmp_path+'all_sorts.csv',)\n",
    "\n",
    "    m2l_utils.plot_points(output_path+'orientations_polarity.csv',geol_clip,'polarity','X','Y',True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate minimum fault offset from stratigraphy and stratigraphic fault offset¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:37:48.111980Z",
     "start_time": "2020-05-21T00:37:48.103984Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['strat_offset']):\n",
    "    m2l_geometry.fault_strat_offset(output_path,c_l,dst_crs,output_path+'formation_summary_thicknesses.csv', tmp_path+'all_sorts.csv',tmp_path+'faults_clip.shp',tmp_path+'geol_clip.shp',output_path+'fault_dimensions.csv')\n",
    "\n",
    "\n",
    "    m2l_utils.plot_points(output_path+'fault_strat_offset3.csv',geol_clip,'min_offset','X','Y',True)\n",
    "    m2l_utils.plot_points(output_path+'fault_strat_offset3.csv',geol_clip,'strat_offset','X','Y',True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse fault-fault topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:37:48.704104Z",
     "start_time": "2020-05-21T00:37:48.113981Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "m2l_topology.parse_fault_relationships(graph_path,tmp_path,output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# loop2gemodeller test \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:38:07.184194Z",
     "start_time": "2020-05-21T00:37:48.706103Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['model_engine']=='geomodeller'):\n",
    "    from datetime import datetime\n",
    "    import shutil\n",
    "    nowtime=datetime.now().isoformat(timespec='minutes')   \n",
    "    model_name=test_data_name+'_'+nowtime.replace(\":\",\"-\").replace(\"T\",\"-\")\n",
    "    os.mkdir(test_data_path+'/'+model_name)\n",
    "    save_faults=True\n",
    "    compute_etc=True\n",
    "    t1 = time.time()\n",
    "    m2l_export.loop2geomodeller(model_name,test_data_path,tmp_path,output_path,'../dtm/dtm_rp.tif',bbox,\n",
    "                                model_top,model_base,save_faults,compute_etc,workflow)\n",
    "    t2 = time.time()\n",
    "    os.chdir(test_data_path+'/'+model_name)\n",
    "    %system geomodellerbatch.exe -batch m2l.taskfile\n",
    "    t3 = time.time()\n",
    "    #%system geomodellerbatch.exe -batch m2l_compute.taskfile\n",
    "    t4 = time.time()\n",
    "    print(\"m2l\",(t1-t0)/60.0,\"export process\",(t2-t1)/60.0,\"batch process\",(t3-t2)/60.0,\"batch calculate\",(t4-t3)/60.0,\"minutes\")\n",
    "    #shutil.copy('../tmp','.')\n",
    "    #shutil.copy('../output','.')\n",
    "    #shutil.copy('../graph','.')\n",
    "    #shutil.copy('../dtm','.')\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loopstructural test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:35:24.836062Z",
     "start_time": "2020-05-21T00:35:24.828061Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['model_engine']=='loopstructural'):\n",
    "    t1 = time.time()\n",
    "    import lavavu\n",
    "    from pyamg import solve\n",
    "\n",
    "    m2l_export.loop2LoopStructural(output_path+'formation_thicknesses.csv',output_path+'orientations.csv',output_path+'contacts4.csv',bbox)\n",
    "    t2 = time.time()\n",
    "    print(\"m2l\",(t1-t0)/60.0,\"LoopStructural\",(t2-t1)/60.0,\"Total\",(t2-t0)/60.0,\"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gempy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:35:24.848064Z",
     "start_time": "2020-05-21T00:35:24.840063Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(workflow['model_engine']=='gempy'):\n",
    "\n",
    "    t1 = time.time()\n",
    "    import importlib\n",
    "    importlib.reload(m2l_export)\n",
    "\n",
    "\n",
    "    vtk=False\n",
    "    m2l_export.loop2gempy(test_data_name,tmp_path,vtk_path,output_path+'orientations_clean.csv',\n",
    "                                    output_path+'contacts_clean.csv',tmp_path+'groups_clean.csv',\n",
    "                                    bbox,model_base, model_top,vtk,dtm_reproj_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noddy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:35:24.865070Z",
     "start_time": "2020-05-21T00:35:24.851066Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['model_engine']=='noddy'):\n",
    "\n",
    "    import pynoddy.history\n",
    "    import networkx as nx\n",
    "    #Read a csv file with the vertices of the faults\n",
    "    #see notes in the bottom of the notebook for instructions on how to generate such vertices files\n",
    "    t1 = time.time()\n",
    "    \n",
    "    scale=1.5 #  scales mdoel to fit predefined volume (complete hack)\n",
    "    \n",
    "    # load fault coordinates\n",
    "    faultsxy=pd.read_csv(output_path+'faults.csv')\n",
    "\n",
    "    #load fault graph, remove cyclic loops and find (non-unique) age-ordered list \n",
    "    G=nx.read_gml(tmp_path+\"fault_network.gml\")\n",
    "    cycles=list(nx.simple_cycles(G))\n",
    "    for c in cycles:\n",
    "        G.remove_edge(c[0], c[1])\n",
    "    faults=nx.topological_sort(G)\n",
    "\n",
    "    # write out Noe format format file\n",
    "    file=open(tmp_path+'faults_for_noe.csv','w')\n",
    "    file.write('id,DipDirecti,X,Y\\n')\n",
    "    for f in faults:\n",
    "            fxy=faultsxy[faultsxy[\"formation\"]==f.replace(\"\\n\",\"\")]\n",
    "            #display(f.replace(\"\\n\",\"\"))\n",
    "            for ind,xy in fxy.iterrows():\n",
    "                ostr=f.replace('\\n','')+',West,'+str(xy['X']/scale)+','+str(xy['Y']/scale)+'\\n'\n",
    "                file.write(ostr)\n",
    "    file.close()\n",
    "    \n",
    "    csvfile = tmp_path+'faults_for_noe.csv'\n",
    "    CsvFaultData = pd.read_csv(csvfile)\n",
    "\n",
    "    #how much does the fault slip relative to the fault length\n",
    "    SlipParam = 0.1\n",
    "\n",
    "    #the xyz origin of the model you will be generating\n",
    "    xy_origin=[minx/scale,miny/scale, 1200-4000]\n",
    "\n",
    "    #Get information about each parameter in Noddy format\n",
    "    #The output from the function is a dictionary with lists of the fault parameters\n",
    "    noddyFormattedFaultData =  pynoddy.history.setUpFaultRepresentation(CsvFaultData,\n",
    "                                                        xy_origin=xy_origin, \n",
    "                                                        SlipParam=SlipParam)\n",
    "\n",
    "    #Create a dictionary with the stratigraphy information\n",
    "    StratDict = {}\n",
    "    StratDict['Heights'] = [2000, 2500, 3000, 3700]\n",
    "    StratDict['Names'] = ['Intrusive', 'Felsic', 'Mafic','Sed'] \n",
    "    StratDict['Density'] =  [2.65, 2.5, 2.4, 2.3] \n",
    "    StratDict['MagSus'] = [0.0015, 0.0012, 0.0018, 0.001]\n",
    "\n",
    "    #Now make the history file\n",
    "    filename = output_path+'faultmodel.his'\n",
    "    noddyFormattedFaultData =  pynoddy.history.createPyNoddyHistoryFile(noddyFormattedFaultData, StratDict, filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T00:35:24.879060Z",
     "start_time": "2020-05-21T00:35:24.868067Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['model_engine']=='noddy'):\n",
    "    import vtkplotter as vtkP\n",
    "    import itkwidgets\n",
    "    import k3d\n",
    "    import pynoddy.output\n",
    "    import pynoddy.history\n",
    "\n",
    "    modelfile = output_path+'faultmodel.his'\n",
    "\n",
    "    # Determine the path to the noddy executable\n",
    "    noddy_path = '../../pynoddy-new/noddyapp/noddy_win64.exe'\n",
    "\n",
    "    # Where you would like to place all your output files\n",
    "    outputfolder = tmp_path\n",
    "\n",
    "    # choose what software to use for visualizing the model\n",
    "    #you can also choose to change to itkwidgets, k3d, False (popup), or panel\n",
    "    #you might need to install packages depending on what you choose\n",
    "    vtkP.settings.embedWindow('k3d') \n",
    "\n",
    "    # create a plot in vtkplotter\n",
    "    plot = vtkP.Plotter(axes=1, bg='white', interactive=1)\n",
    "\n",
    "    # call the plotting function\n",
    "    points = pynoddy.output.CalculatePlotStructure(modelfile, plot, noddy_path, \n",
    "                                           outputfolder=outputfolder,\n",
    "                                           LithologyOpacity=0.2, outputOption=0)\n",
    "    plot.show(viewup='z')\n",
    "    t2 = time.time()\n",
    "\n",
    "    print(\"m2l\",(t1-t0)/60.0,\"noddy\",(t2-t1)/60.0,\"Total\",(t2-t0)/60.0,\"minutes\")\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
