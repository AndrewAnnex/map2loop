{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map2loop: From geology layers to outputs to various 3D modelling programs- Hamersley example\n",
    "\n",
    "This notebook reads in three layers from  local or remote sources:  geology polygons, orientation data and fault polylines; and calculates the topological relationships between the different features. Requires compiled cpp code from Vitaliy Ogarko\n",
    "\n",
    "This all gets fed into successive tolopogical and geometric transfroms that end up feeding into gempy to make a 3D model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T07:14:20.987809Z",
     "start_time": "2019-11-08T07:14:19.452748Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import stat\n",
    "import functools \n",
    "import operator  \n",
    "import matplotlib\n",
    "import networkx as nx\n",
    "import rasterio\n",
    "from shapely.geometry import Polygon\n",
    "from map2loop import m2l_utils\n",
    "from map2loop import m2l_topology\n",
    "from map2loop import m2l_geometry\n",
    "from map2loop import m2l_interpolation\n",
    "%matplotlib inline\n",
    "\n",
    "#newwd=\"C:\\\\Users\\\\00073294\\Dropbox\\\\loop_minex\\\\map2model\\\\\"\n",
    "#os.chdir(newwd)\n",
    "print(\"Current Working Directory \" )\n",
    "\n",
    "gdal_data = os.environ['GDAL_DATA']\n",
    "print(\"***\",gdal_data)\n",
    "print('is dir: ' + str(os.path.isdir(gdal_data)))\n",
    "gcs_csv = os.path.join(gdal_data, 'gcs.csv')\n",
    "print('is file: ' + str(os.path.isfile(gcs_csv)))\n",
    "st = os.stat(gcs_csv)\n",
    "print('is readable: ' + str(bool(st.st_mode & stat.S_IRGRP)))\n",
    "os.environ['PROJ_LIB']=r\"C:\\\\Users\\\\00073294\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\Lib\\\\site-packages\\\\pyproj\\\\proj_dir\\\\share\\\\proj\"\n",
    "print(os.getenv('PROJ_LIB'))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bounding box based on inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:42.392657Z",
     "start_time": "2019-11-07T06:34:42.379682Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data_name='test_data3'\n",
    "\n",
    "test_data_path='../'+test_data_name+'/'\n",
    "\n",
    "os.chdir(test_data_path)\n",
    "%run -i \"m2l_config.py\"\n",
    "#%run -i \"../test_data3/m2l_config.py\"\n",
    "print(os.getcwd())\n",
    "\n",
    "bbox2=str(minx)+\",\"+str(miny)+\",\"+str(maxx)+\",\"+str(maxy)\n",
    "lat_point_list = [miny, miny, maxy, maxy, maxy]\n",
    "lon_point_list = [minx, maxx, maxx, minx, minx]\n",
    "bbox_geom = Polygon(zip(lon_point_list, lat_point_list))\n",
    "polygon = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "bbox=(minx,miny,maxx,maxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:42.397651Z",
     "start_time": "2019-11-07T06:34:42.393634Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fold_decimate=2         \n",
    "\n",
    "contact_decimate=20\n",
    "\n",
    "use_interpolations=True       #use interpolated dips/contacts as additional constraints\n",
    "\n",
    "use_fat=True                   #use fold axial trace orientation hints\n",
    "\n",
    "    \n",
    "local_paths=True              #use local copies of data\n",
    "\n",
    "pluton_form='domes'\n",
    "\n",
    "min_fault_length=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we test to see if we have access to the online data we need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:44.181685Z",
     "start_time": "2019-11-07T06:34:42.399618Z"
    }
   },
   "outputs": [],
   "source": [
    "loopwfs=m2l_utils.have_access(\"geo.loop-gis.org\")\n",
    "hawaii=m2l_utils.have_access(\"pae-paha.pacioos.hawaii.edu\")\n",
    "\n",
    "if(not (loopwfs & hawaii)):\n",
    "    local_paths=True\n",
    "    net=False\n",
    "    print('using local paths')\n",
    "else:\n",
    "    net=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional WFS source\n",
    "  \n",
    "WFS brings in field names as lower case, so need to redefine codes too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:44.188695Z",
     "start_time": "2019-11-07T06:34:44.182682Z"
    }
   },
   "outputs": [],
   "source": [
    "if((not local_paths) and net):\n",
    "    structure_file='http://geo.loop-gis.org/geoserver/loop/wfs?service=WFS&version=1.1.0&request=GetFeature&typeName=warox_points_f5011&bbox='+bbox2+'&srs=EPSG:28350'\n",
    "    fault_file='http://geo.loop-gis.org/geoserver/loop/wfs?service=WFS&version=1.1.0&request=GetFeature&typeName=linear_500k&bbox='+bbox2+'&srs=EPSG:28350'\n",
    "    geology_file='http://geo.loop-gis.org/geoserver/loop/wfs?service=WFS&version=1.0.0&request=GetFeature&typeName=loop:geol_500k&bbox='+bbox2+'&srs=EPSG:28350'\n",
    "\n",
    "    c_l= {\n",
    "    #Orientations\n",
    "      \"d\": \"dip\",                  #field that contains dip information\n",
    "      \"dd\": \"dip_dir\",             #field that contains dip direction information\n",
    "      \"sf\": 'feature',             #field that contains information on type of structure\n",
    "      \"bedding\": 'Bed',            #text to search for in field defined by sfcode to show that this is a bedding measurement\n",
    "    #Stratigraphy\n",
    "      \"g\": 'group_',               #field that contains coarser stratigraphic coding\n",
    "      \"c\": 'code',                 #field that contains finer stratigraphic coding\n",
    "      \"ds\": 'descriptn',           #field that contains information about lithology\n",
    "      \"u\": 'unitname',             #field that contains alternate stratigraphic coding (not used??)\n",
    "      \"r1\": 'rocktype1',           #field that contains  extra lithology information\n",
    "      \"r2\": 'rocktype2',           #field that contains even more lithology information\n",
    "      \"sill\": 'sill',              #text to search for in field defined by dscode to show that this is a sill\n",
    "      \"intrusive\": 'intrusive',    #text to search for in field defined by dscode to show that this is an intrusion\n",
    "      \"volcanic\": 'volcanic',      #text to search for in field defined by dscode to show that this is an intrusion\n",
    "    #Timing\n",
    "      \"min\": 'min_age_ma',         #field that contains minimum age of unit defined by ccode\n",
    "      \"max\": 'max_age_ma',         #field that contains maximum age of unit defined by ccode\n",
    "    #faults and folds\n",
    "      \"f\": 'feature',              #field that contains information on type of structure\n",
    "      \"fault\": 'Fault',            #text to search for in field defined by fcode to show that this is a fault\n",
    "      \"fold\": 'Fold axial trace',  #text to search for in field defined by fcode to show that this is a fold axial trace\n",
    "      \"n\": 'name',                 #field that contains information on name of fault (not used??)\n",
    "      \"t\": 'type',                 #field that contains information on type of fold\n",
    "    #ids\n",
    "      \"o\": 'objectid',             #field that contains unique id of geometry object\n",
    "      \"gi\": 'geopnt_id'            #field that contains unique id of structure point\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot geology polygons and bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:44.844913Z",
     "start_time": "2019-11-07T06:34:44.189664Z"
    }
   },
   "outputs": [],
   "source": [
    "print(geology_file)\n",
    "geology_ll = gpd.read_file(geology_file,bbox=bbox)\n",
    "\n",
    "base=geology_ll.plot(column=c_l['c'],figsize=(10,10),edgecolor='#000000',linewidth=0.2)\n",
    "polygon.plot(ax=base, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to file as WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:45.452305Z",
     "start_time": "2019-11-07T06:34:44.845910Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_geol = geology_ll[['geometry', c_l['o'],c_l['c'],c_l['g'],c_l['u'],c_l['min'],c_l['max'],c_l['ds'],c_l['r1'],c_l['r2']]]\n",
    "m2l_topology.save_geol_wkt(sub_geol,geology_file_csv, c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and save WAROX point data as WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:45.887304Z",
     "start_time": "2019-11-07T06:34:45.452305Z"
    }
   },
   "outputs": [],
   "source": [
    "warox = gpd.read_file(structure_file,bbox=bbox)\n",
    "\n",
    "sub_pts = warox[['geometry', c_l['gi'],c_l['d'],c_l['dd']]]\n",
    "\n",
    "m2l_topology.save_structure_wkt(sub_pts,structure_file_csv,c_l)\n",
    "\n",
    "base=sub_pts.plot()\n",
    "polygon.plot(ax=base, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot faults and bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:46.211585Z",
     "start_time": "2019-11-07T06:34:45.889299Z"
    }
   },
   "outputs": [],
   "source": [
    "lines_ll=gpd.read_file(fault_file,bbox=bbox)\n",
    "\n",
    "base2=lines_ll.plot(cmap='rainbow',column=c_l['f'],figsize=(10,10),linewidth=0.4)\n",
    "polygon.plot(ax=base2, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save faults to file as WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:46.303340Z",
     "start_time": "2019-11-07T06:34:46.212582Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_lines = lines_ll[['geometry', c_l['o'],c_l['f']]]\n",
    "m2l_topology.save_faults_wkt(sub_lines,fault_file_csv,c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create map2model input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:46.310321Z",
     "start_time": "2019-11-07T06:34:46.305334Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_topology.save_Parfile(m2m_cpp_path,c_l,graph_path,geology_file_csv,fault_file_csv,structure_file_csv,minx,maxx,miny,maxy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:48.486049Z",
     "start_time": "2019-11-07T06:34:46.312315Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.chdir(m2m_cpp_path)\n",
    "print(os.getcwd())\n",
    "%system map2model.exe Parfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple network graph of the geology with legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:49.227950Z",
     "start_time": "2019-11-07T06:34:48.488069Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "G=nx.read_gml(strat_graph_file,label='id')\n",
    "selected_nodes = [n for n,v in G.nodes(data=True) if n >=0]\n",
    "nx.draw_networkx(G, pos=nx.kamada_kawai_layout(G), arrows=True, nodelist=selected_nodes)\n",
    "\n",
    "nlist=list(G.nodes.data('LabelGraphics'))\n",
    "nlist.sort()\n",
    "for no in nlist:\n",
    "    if(no[0]>=0):\n",
    "        elem=str(no[1]).replace(\"{'text':\",\"\").replace(\", 'fontSize': 14}\",\"\")\n",
    "        #second=elem.split(\":\").replace(\"'\",\"\")\n",
    "        print(no[0],\" \",elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Topology via dot or gml format files\n",
    "For a more detailed look we can open up the online version of <b>yEd</b> <a href=\"https://www.yworks.com/yed-live/\" >https://www.yworks.com/yed-live/</a>  and load in the *.dot file that we will now create or dowload the <a href=\"https://www.yworks.com/products/yed/download\">yEd program</a> and look at the *.gml files in the <b>output</b> directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from networkx.drawing.nx_pydot import write_dot\n",
    "\n",
    "import filer,gfiler\n",
    "\n",
    "filepath=filer.gui_fname().decode('UTF-8')\n",
    "write_dot(G,filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8888/notebooks/Dropbox/1_Jupyter_notebooks/map2loop/notebooks/2.%20map2loop_after_topology.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Process topography, stratigraphy, fold axial traces and faults\n",
    "\n",
    "### Takes GML file produced by topology code, combines with geology polygons, structure points and dtm to create 3D model in gempy.<br><br>\n",
    "\n",
    "Limitations:  no dykes, no sills. Sills require us to assign a unique surface to each instance of a sill (sill between units A and B needs to be different from sill of same age and strat codes as one found between E and F). Dykes via cokriging are really hard without just cookie cutting them in (but that is not our problem!). We are not checking for onlap relationships, which can perhaps been seen by having lots of units from one series adjacent to the youngest surface of the older series. Could also think about interpreting these as faults to introduce conceptual uncertainty. All mistakes belong to Mark Jessell, topology code that feeds this system by Vitaliy Ogarko.<br><br>\n",
    "\n",
    "Geology layer needs to have some unique strat code or text, some group code or text to function<br>\n",
    "Structure layer needs dip/dip direction<br>\n",
    "\n",
    "<font color='red'>Currently mostly hardwired to GSWA 500K map so needs work...</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:49.233933Z",
     "start_time": "2019-11-07T06:34:49.228948Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('../map2loop')\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:49.241913Z",
     "start_time": "2019-11-07T06:34:49.235927Z"
    }
   },
   "outputs": [],
   "source": [
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "sys.path.insert(0,\"../..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "#print(os.getcwd())\n",
    "#os.environ[\"PROJ_LIB\"] = r\"C:\\Users\\00073294\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\pyproj\\proj_dir\\share\\proj\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we define an area of interest and some other basic stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:49.251886Z",
     "start_time": "2019-11-07T06:34:49.243906Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "#test_data_path='../test_data3/'\n",
    "\n",
    "\n",
    "#%run -i \"../test_data3/m2l_config.py\"\n",
    "\n",
    "bbox2=str(minx)+\",\"+str(miny)+\",\"+str(maxx)+\",\"+str(maxy)\n",
    "lat_point_list = [miny, miny, maxy, maxy, maxy]\n",
    "lon_point_list = [minx, maxx, maxx, minx, minx]\n",
    "bbox_geom = Polygon(zip(lon_point_list, lat_point_list))\n",
    "polygon = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "bbox=(minx,miny,maxx,maxy)\n",
    "\n",
    "step_out=0.045 #add (in degrees) so edge pixel from dtm reprojection are not found\n",
    "\n",
    "\n",
    "#contact_decimate=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Download and reproject the appropriate SRTM data\n",
    "mj: Getting this from GA, but could also get from Hawaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:49.591220Z",
     "start_time": "2019-11-07T06:34:49.254877Z"
    }
   },
   "outputs": [],
   "source": [
    "polygon_ll=polygon.to_crs(src_crs)\n",
    "\n",
    "minlong=polygon_ll.total_bounds[0]-step_out\n",
    "maxlong=polygon_ll.total_bounds[2]+step_out\n",
    "minlat=polygon_ll.total_bounds[1]-step_out\n",
    "maxlat=polygon_ll.total_bounds[3]+step_out\n",
    "\n",
    "print(minlong,maxlong,minlat,maxlat)\n",
    "if((not local_paths) and net):     \n",
    "    m2l_utils.get_dtm(dtm_file, minlong,maxlong,minlat,maxlat)\n",
    "    geom_rp=m2l_utils.reproject_dtm(dtm_file,dtm_reproj_file,src_crs,dst_crs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Load stratigraphy graph and create list of series (aka groups)\n",
    "mj: The choice of what constitutes basic unit and what a group of units is hard-wired at the moment, but could be altered to any pair. Not even sure we need two levels but it seemed like a good idea at the time. Note that this needs the arcgis plugin version of the topology code (for now) as it seperates the different sub graphs. Text outputs list alternate topologies for series and surfaces, which if confirmed by comapring max-min ages will be a nice source of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:52.346432Z",
     "start_time": "2019-11-07T06:34:49.594212Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groups,glabels,G = m2l_topology.get_series(strat_graph_file,'id')\n",
    "m2l_topology.save_units(G,tmp_path,glabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Load geology & structure data\n",
    "Currently loading from local files, but could load geology from WFS server at GSWA EXCEPT that the WFS online map has less fields that the zipped shapefiles. Go figure. We don't use fault layer at the moment (except for Vitaliy's topology code) but same logic applies in terms of where to get it from. Already have fault/strat relationships and once we have fault/fault relationships will start to include faults in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:53.014406Z",
     "start_time": "2019-11-07T06:34:52.348428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract point data from structure & geology layers for modelling\n",
    "##First we readin the structure and map from shapefiles, or wherever...\n",
    "\n",
    "bbox=(minx,miny,maxx,maxy)\n",
    "geology = gpd.read_file(geology_file,bbox=bbox)\n",
    "\n",
    "\n",
    "structure = gpd.read_file(structure_file,bbox=bbox)\n",
    "structure.crs=dst_crs\n",
    "print(fault_file)\n",
    "faults = gpd.read_file(fault_file,bbox=bbox)\n",
    "faults.crs=dst_crs\n",
    "#display(faults)\n",
    "sub_pts = structure[['geometry',c_l['d'],c_l['dd'],c_l['sf']]] \n",
    "\n",
    "base=geology.plot(column=c_l['c'],figsize=(10,10),edgecolor='#000000',linewidth=0.2)\n",
    "sub_pts.plot(ax=base,edgecolor='black')\n",
    "faults.plot(ax=base, column=c_l['f'],edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Clip geology, faults, structures and map geology to structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:55.611972Z",
     "start_time": "2019-11-07T06:34:53.016401Z"
    }
   },
   "outputs": [],
   "source": [
    "geology = m2l_utils.explode(geology)\n",
    "geology.crs = dst_crs\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "structure_code = gpd.sjoin(sub_pts, geology, how=\"left\", op=\"within\")\n",
    "\n",
    "y_point_list = [miny, miny, maxy, maxy, miny]\n",
    "x_point_list = [minx, maxx, maxx, minx, minx]\n",
    "\n",
    "bbox_geom = Polygon(zip(x_point_list, y_point_list))\n",
    "\n",
    "polygo = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "#display(polygo.geometry)\n",
    "is_bed=structure_code[c_l['sf']].str.contains(c_l['bedding'], regex=False) \n",
    "    \n",
    "all_beds = structure_code[is_bed]\n",
    "#display(sfcode)\n",
    "all_folds=faults[faults[c_l['f']].str.contains(c_l['fold'])]\n",
    "folds_clip=m2l_utils.clip_shp(all_folds,polygo)\n",
    "folds_clip.to_file(tmp_path+'folds_clip.shp')\n",
    "\n",
    "\n",
    "all_faults=faults[faults[c_l['f']].str.contains(c_l['fault'])]\n",
    "\n",
    "\n",
    "\n",
    "#display(structure_code)\n",
    "geol_clip=m2l_utils.clip_shp(geology, polygo)\n",
    "faults_clip=m2l_utils.clip_shp(all_faults,polygo)\n",
    "#display(faults_clip)\n",
    "structure_clip = m2l_utils.clip_shp(all_beds, polygo)\n",
    "\n",
    "base = geol_clip.plot(column=c_l['c'],figsize=(7,7),edgecolor='#000000',linewidth=0.2)\n",
    "faults_clip.plot(ax=base, column=c_l['t'],edgecolor='black')\n",
    "structure_clip.plot(ax=base, column=c_l['c'],edgecolor='black')\n",
    "\n",
    "geol_clip.to_file(tmp_path+'geol_clip.shp')\n",
    "faults_clip.to_file(tmp_path+'faults_clip.shp')\n",
    "structure_clip.to_file(tmp_path+'structure_clip.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create possible stratigraphy sets per group\n",
    "mj: <font color='red'>Uses first of each possible set of toplogies per unit and per group, which is arbitrary. </font>On the other hand we are not checking relative ages again to see if this helps reduce ambiguity, which I think it would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:56.057071Z",
     "start_time": "2019-11-07T06:34:55.613967Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2l_topology.save_group(G,tmp_path,glabels,geol_clip,c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Export orientation data in csv  format\n",
    "mj: Orientation data needs calculated height as file does not provide it, taken from SRTM data already downloaded. To calculate polarity <font color='red'>(WHICH WE DON'T DO YET)</font> we can calculate the dot product of the dip direction of a bedding plane and the vector to that points nearest basal contact node, if  abs(acos(dot product))>90  then right way up :\n",
    "\n",
    "<img src='../graphics/polarity.png'>\n",
    "\n",
    "Added code to not save intrusion orientation data as they won't have associated surfaces if sill..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:56.129877Z",
     "start_time": "2019-11-07T06:34:56.058036Z"
    }
   },
   "outputs": [],
   "source": [
    "dtm = rasterio.open(dtm_reproj_file)\n",
    "m2l_geometry.save_orientations(structure_code,output_path,c_l,orientation_decimate,dtm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Find those series that don't have any orientation or contact point data  then create arbitrary point for series with no orientation data\n",
    "Not sure if gempy needs this but geomodeller does. Currently just gives a point dipping 45 degrees to North, but could use dip direction normal to basal surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:34:56.402117Z",
     "start_time": "2019-11-07T06:34:56.133866Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2l_geometry.create_orientations( tmp_path, output_path, dtm,geol_clip,structure_clip,c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Export contact information subset of each polygon to gempy format\n",
    "mj: Orientation data needs calculated height as file does not provide it, taken from SRTM data already downloaded. Need to reduce number of points whilst retaining useful info (Ranee's job!)'\n",
    "To calculate which are the basal units contact for a polygon find the polygons which are older than the selected polygon, in the example below the central polygon has relative age 23 so its basal contact is with the polygons whose ages are 26 & 28. If there are no older units for a polygon it has no basal content. We keep every nth node based on the decimate term (simple count along polyline). gempy seems to need at least two points per surface, so we always take the first two points.\n",
    "\n",
    "\n",
    "<img src='../graphics/base.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:06.234072Z",
     "start_time": "2019-11-07T06:34:56.403115Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls_dict,ls_dict_decimate=m2l_geometry.save_basal_contacts(tmp_path,dtm,geol_clip,contact_decimate,c_l,intrusion_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all basal contacts that are defined by faults and save to shapefile (no decimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:06.307873Z",
     "start_time": "2019-11-07T06:35:06.237063Z"
    }
   },
   "outputs": [],
   "source": [
    "display(ls_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:08.028084Z",
     "start_time": "2019-11-07T06:35:06.311863Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_geometry.save_basal_no_faults(tmp_path+'basal_contacts.shp',tmp_path+'faults_clip.shp',ls_dict,10,c_l,dst_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove faults from decimated basal contacts as save as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:09.495401Z",
     "start_time": "2019-11-07T06:35:08.029082Z"
    }
   },
   "outputs": [],
   "source": [
    "contacts=gpd.read_file(tmp_path+'basal_contacts.shp')\n",
    "\n",
    "m2l_geometry.save_basal_contacts_csv(contacts,output_path,dtm,contact_decimate)\n",
    "\n",
    "#m2l_geometry.save_contacts_with_faults_removed(tmp_path+'faults_clip.shp',output_path,10,ls_dict,ls_dict_decimate,c_l,dst_crs,dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save fault as contact info and and orientation info make vertical (for the moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:09.756700Z",
     "start_time": "2019-11-07T06:35:09.497395Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_geometry.save_faults(tmp_path+'faults_clip.shp',output_path,dtm,c_l,fault_decimate,min_fault_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create basal contact points with orientation from orientations and basal points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contacts = gpd.read_file(tmp_path+'basal_contacts2.shp') #load orientation data as geopandas dataframe \n",
    "structures = gpd.read_file(tmp_path+'structure_clip.shp') #load orientation data as geopandas dataframe \n",
    "\n",
    "m2l_geometry.create_basal_contact_orientations(contacts,structures,output_path,dtm,2500,ccode,gcode,dcode,ddcode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates fold axial trace points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:09.857431Z",
     "start_time": "2019-11-07T06:35:09.758697Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_geometry.save_fold_axial_traces(tmp_path+'faults_clip.shp',output_path,dtm,c_l,fold_decimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process plutons\n",
    "\n",
    "For each instruve but not sill polygon, find older neighbours and store decimated contact points. Also store dipping contact orientations (user defined, just because) with four possible sub-surface configurations:\n",
    "\n",
    "<b>saucers: \\\\_+++_/ <br>\n",
    "batholiths: +++/__ __ _\\\\+++  <br> \n",
    "domes: /‾+++‾\\\\ <br>\n",
    "pendants: +++\\\\_  _/+++ <br>\n",
    "</b>\n",
    "  \n",
    "Saves out orientations and contact points, as well as updated group level stratigraphic column.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:11.856919Z",
     "start_time": "2019-11-07T06:35:09.858429Z"
    }
   },
   "outputs": [],
   "source": [
    "bbox=(minx,miny,maxx,maxy)\n",
    "\n",
    "pluton_dip=str(pluton_dip)\n",
    "\n",
    "dist_buffer=10\n",
    "dtm = rasterio.open(dtm_reproj_file)\n",
    "m2l_geometry.process_plutons(tmp_path,output_path,geol_clip,local_paths,dtm,pluton_form,pluton_dip,contact_decimate,c_l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data to ensure it meets gempy's requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T01:40:10.318904Z",
     "start_time": "2019-11-06T01:40:09.854171Z"
    }
   },
   "source": [
    "if(local_paths): ###############FUDGE#############\n",
    "    use_gcode=('Hamersley_Group','Fortescue_Group','Wyloo_Group','Shingle_Creek_Group','Turee_Creek_Group','A_mgn_PRK',  'A_mgn_PMI' ) ################# MOVE UP   #########################\n",
    "else:\n",
    "    use_gcode=('Hamersley_Group','Fortescue_Group','Wyloo_Group','Shingle_Creek_Group','Turee_Creek_Group','A_mgn_PMI_7366',  'A_mgn_PRK_16193' ) ################# MOVE UP   #########################\n",
    "\n",
    "\n",
    "print('only processing',use_gcode)\n",
    "\n",
    "m2l_geometry.tidy_data(output_path,tmp_path,use_gcode,use_interpolations,pluton_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T01:42:55.068408Z",
     "start_time": "2019-11-06T01:42:55.019505Z"
    }
   },
   "source": [
    "m2l_topology.parse_fault_relationships(graph_path,tmp_path,output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolates a regular grid of orientations from an  shapefile of arbitrarily-located points and saves out four csv files of l,m & n direction cosines and dip dip direction data\n",
    "\n",
    "Can choose between various RBF and IDW options   \n",
    "  \n",
    "The purpose of these interpolations and associated code is to help in three cases:\n",
    "- Providing estimated dips and contacts in fault-bounded domains where no structural data are available\n",
    "- Needed to estimate true thickness of formations\n",
    "- Possibly useful for poulating parts of maps where little structural data is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:11.862977Z",
     "start_time": "2019-11-07T06:35:11.858913Z"
    }
   },
   "outputs": [],
   "source": [
    "use_gcode=('Hamersley Group','Fortescue Group','Wyloo Group','Shingle Creek Group','Turee Creek Group' ) ################# MOVE UP   #########################\n",
    "structure_file=tmp_path+'structure_clip.shp'\n",
    "bbox=(minx+inset,miny+inset,maxx-inset,maxy-inset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:11.871911Z",
     "start_time": "2019-11-07T06:35:11.864898Z"
    }
   },
   "outputs": [],
   "source": [
    "print(c_l['g'])\n",
    "print(use_gcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:13.710688Z",
     "start_time": "2019-11-07T06:35:11.872875Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_interpolation.interpolate_orientations(structure_file,tmp_path,bbox,c_l,use_gcode,scheme,gridx,gridy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolates a regular grid from a shapefile of from contacts and saves out three csv files of l & m direction cosines and strike data\n",
    "\n",
    "Can choose between various RBF and IDW options  \n",
    "\n",
    "<font face color='red'>Best with basal contacts only, using Notebook #1 as this avoids intrusive  and fault contacts being considered</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:13.717670Z",
     "start_time": "2019-11-07T06:35:13.712683Z"
    }
   },
   "outputs": [],
   "source": [
    "geology_file=tmp_path+'basal_contacts.shp'\n",
    "\n",
    "dtm = rasterio.open(dtm_reproj_file)\n",
    "use_gcode=('Hamersley_Group','Fortescue_Group','Wyloo_Group','Shingle_Creek_Group','Turee_Creek_Group' ) ################# MOVE UP   #########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:52.834364Z",
     "start_time": "2019-11-07T06:35:13.719665Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_interpolation.interpolate_contacts(geology_file,tmp_path,dtm,bbox,c_l,use_gcode,scheme,gridx,gridy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combines rbf of contact orientations with rbf of dip/dipdir measurements\n",
    "\n",
    "Uses normalised direction cosines **(l,m,n)**:  \n",
    "- **l,m** from RBF of basal contact orientations  \n",
    "- **signs of l & m** from misorientation with RBF of orientation data and  \n",
    "- **n** from RBF of orientation data  \n",
    "  \n",
    "May be useful for adding data where no orientations are available (e.g. in fault bounded domains) and for calculating true thickness of layers. Assumes a 2D plane of data, but if 3D RBF was calulated and projected contact info was used it should apply with topography too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:53.002916Z",
     "start_time": "2019-11-07T06:35:52.835362Z"
    }
   },
   "outputs": [],
   "source": [
    "geology_file=tmp_path+'geol_clip.shp'\n",
    "combo_file=tmp_path+'combo.csv'\n",
    "\n",
    "lc=np.loadtxt(tmp_path+'interpolation_contacts_l.csv',skiprows =1,delimiter =',',dtype=float)\n",
    "mc=np.loadtxt(tmp_path+'interpolation_contacts_m.csv',skiprows =1,delimiter =',',dtype=float)\n",
    "lo=np.loadtxt(tmp_path+'interpolation_l.csv',skiprows =1,delimiter =',',dtype=float)\n",
    "mo=np.loadtxt(tmp_path+'interpolation_m.csv',skiprows =1,delimiter =',',dtype=float)\n",
    "no=np.loadtxt(tmp_path+'interpolation_n.csv',skiprows =1,delimiter =',',dtype=float)\n",
    "xy=np.loadtxt(tmp_path+'interpolation_'+scheme+'.csv',skiprows =1,delimiter =',',dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:35:55.919455Z",
     "start_time": "2019-11-07T06:35:53.004910Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_interpolation.join_contacts_and_orientations(combo_file,geology_file,tmp_path,dtm_reproj_file,c_l,lo,mo,no,lc,mc,xy,dst_crs,bbox)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data to ensure it meets gempy's requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:36:01.457792Z",
     "start_time": "2019-11-07T06:35:55.921449Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(local_paths): ###############FUDGE#############\n",
    "    use_gcode=('Hamersley_Group','Fortescue_Group','Wyloo_Group','Shingle_Creek_Group','Turee_Creek_Group','A_mgn_PRK',  'A_mgn_PMI' ) ################# MOVE UP   #########################\n",
    "else:\n",
    "    use_gcode=('Hamersley_Group','Fortescue_Group','Wyloo_Group','Shingle_Creek_Group','Turee_Creek_Group','A_mgn_PMI_7366',  'A_mgn_PRK_16193' ) ################# MOVE UP   #########################\n",
    "\n",
    "\n",
    "print('only processing',use_gcode)\n",
    "\n",
    "m2l_geometry.tidy_data(output_path,tmp_path,use_gcode,use_interpolations,use_fat,pluton_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse fault-fault topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:36:02.954161Z",
     "start_time": "2019-11-07T06:36:01.457792Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_topology.parse_fault_relationships(graph_path,tmp_path,output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# loop2gempy test \n",
    "\n",
    "\n",
    "<font face color='red'>The way this loads data to gempy is not right, so all the units end up in one series... but it gives the basic idea </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Now we define the basic model and load the orientation and lithological information\n",
    "mj: We inset the model dimensions as once reprojects the edge pixels of the topography are undefined. Probably doesn't need to be that extreme (unit is metres). Could use parameter <b>maxtopo</b> to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:36:09.920370Z",
     "start_time": "2019-11-07T06:36:02.956157Z"
    }
   },
   "outputs": [],
   "source": [
    "import gempy as gp\n",
    "\n",
    "geo_model = gp.create_model(test_data_name) \n",
    "\n",
    "gp.init_data(geo_model, extent=[minx, maxx, miny, maxy, model_base, model_top],\n",
    "    resolution = (50,50,50), \n",
    "      #path_o = './test_data/output/hams2_orientations.txt',\n",
    "      path_o = output_path+'orientations_clean.csv',\n",
    "      path_i = output_path+'contacts_clean.csv', default_values=True); #%%      \n",
    "      #path_o = './test_data3/output/ign_orientations_saucers.csv',\n",
    "      #path_i = './test_data3/output/ign_contacts.csv', default_values=True); #%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Show example lithological points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:36:09.959366Z",
     "start_time": "2019-11-07T06:36:09.920370Z"
    }
   },
   "outputs": [],
   "source": [
    "gp.get_data(geo_model, 'surface_points').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Show example orientations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:36:09.978714Z",
     "start_time": "2019-11-07T06:36:09.960360Z"
    }
   },
   "outputs": [],
   "source": [
    "gp.get_data(geo_model, 'orientations').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Plot some of this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:36:10.529577Z",
     "start_time": "2019-11-07T06:36:09.979714Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "gp.plot.plot_data(geo_model, direction='z');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Load reprojected topgraphy to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:36:11.263482Z",
     "start_time": "2019-11-07T06:36:10.530574Z"
    }
   },
   "outputs": [],
   "source": [
    "fp = dtm_reproj_file\n",
    "print(fp)\n",
    "geo_model.set_topography(source='gdal',filepath=fp)\n",
    "\n",
    "## Load stratigraphic Series\n",
    "\n",
    "#f=open(tmp_path+'groups_clean.csv',\"r\")\n",
    "#contents =f.readlines()\n",
    "#f.close\n",
    "#ngroups=contents[0].split(\" \")\n",
    "#ngroups=int(ngroups[1])\n",
    "\n",
    "contents=np.genfromtxt(tmp_path+'groups_clean.csv',delimiter=',',dtype='U25')\n",
    "ngroups=len(contents)\n",
    "\n",
    "faults = gp.Faults()\n",
    "series = gp.Series(faults)\n",
    "series.df\n",
    "\n",
    "#display(ngroups,contents)\n",
    "groups=[]\n",
    "\n",
    "for i in range (0,ngroups):\n",
    "    groups.append(contents[i].replace(\"\\n\",\"\"))\n",
    "    series.add_series(contents[i].replace(\"\\n\",\"\"))\n",
    "    print(contents[i].replace(\"\\n\",\"\"))\n",
    "\n",
    "series.delete_series('Default series')\n",
    "\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Load surfaces and assign to series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:36:12.399498Z",
     "start_time": "2019-11-07T06:36:11.265477Z"
    }
   },
   "outputs": [],
   "source": [
    "surfaces = gp.Surfaces(series)\n",
    "\n",
    "print(ngroups,groups)\n",
    "for i in range(0,ngroups):\n",
    "    contents=np.genfromtxt(tmp_path+groups[i]+'.csv',delimiter=',',dtype='U25')\n",
    "    nformations=len(contents.shape)\n",
    "\n",
    "    if(nformations==1):\n",
    "        for j in range (1,len(contents)):\n",
    "            surfaces.add_surface(str(contents[j]).replace(\"\\n\",\"\"))\n",
    "            d={groups[i]:str(contents[j]).replace(\"\\n\",\"\")}\n",
    "            surfaces.map_series({groups[i]:(str(contents[j]).replace(\"\\n\",\"\"))}) #working but no gps       \n",
    "    else:\n",
    "        #print('lc',len(contents[0]))\n",
    "        for j in range (1,len(contents[0])):\n",
    "            surfaces.add_surface(str(contents[0][j]).replace(\"\\n\",\"\"))\n",
    "            d={groups[i]:str(contents[0][j]).replace(\"\\n\",\"\")}\n",
    "            surfaces.map_series({groups[i]:(str(contents[0][j]).replace(\"\\n\",\"\"))}) #working but no gps\n",
    "\n",
    "\n",
    "surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Set Interpolation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:36:12.421439Z",
     "start_time": "2019-11-07T06:36:12.400495Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "id_only_one_bool = geo_model.surface_points.df['id'].value_counts() == 1\n",
    "id_only_one = id_only_one_bool.index[id_only_one_bool]\n",
    "single_vals = geo_model.surface_points.df[geo_model.surface_points.df['id'].isin(id_only_one)]\n",
    "for idx, vals in single_vals.iterrows():\n",
    "    geo_model.add_surface_points(vals['X'], vals['Y'], vals['Z'], vals['surface'])\n",
    "    \n",
    "geo_model.update_structure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:36:29.759071Z",
     "start_time": "2019-11-07T06:36:12.422444Z"
    }
   },
   "outputs": [],
   "source": [
    "gp.set_interpolation_data(geo_model,\n",
    "                          compile_theano=True,\n",
    "                          theano_optimizer='fast_compile',\n",
    "                          verbose=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Provide summary data on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:36:29.769041Z",
     "start_time": "2019-11-07T06:36:29.761064Z"
    }
   },
   "outputs": [],
   "source": [
    "geo_model.additional_data.structure_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Calculate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:38:24.207580Z",
     "start_time": "2019-11-07T06:36:29.770039Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gp.compute_model(geo_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Extract surfaces to visualize in 3D renderers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:38:24.714278Z",
     "start_time": "2019-11-07T06:38:24.209575Z"
    }
   },
   "outputs": [],
   "source": [
    "gp.plot.plot_section(geo_model, 49, direction='z', show_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:38:25.029469Z",
     "start_time": "2019-11-07T06:38:24.716246Z"
    }
   },
   "outputs": [],
   "source": [
    "ver , sim = gp.get_surfaces(geo_model)\n",
    "\n",
    "import winsound\n",
    "duration = 300  # milliseconds\n",
    "freq = 500  # Hz\n",
    "winsound.Beep(freq, duration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Visualise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:38:43.931178Z",
     "start_time": "2019-11-07T06:38:25.031437Z"
    }
   },
   "outputs": [],
   "source": [
    "gp.plot.plot_3D(geo_model, render_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T06:38:43.937374Z",
     "start_time": "2019-11-07T06:38:43.933389Z"
    }
   },
   "outputs": [],
   "source": [
    "from gempy import plot\n",
    "#gp.plot.export_to_vtk(geo_model, path='../test_data3/vtk/', name='hams3.vtk', voxels=False, block=None, surfaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
