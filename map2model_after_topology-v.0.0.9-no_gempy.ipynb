{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# map2model v0.0.9\n",
    "## Takes GML file produced by topology code, combines with geology polygons, structure points and dtm to create 3D model in gempy.<br><br>\n",
    "\n",
    "Limitations: no faults yet, no plutons yet, no dykes, no sills. Faults and plutons will come soon, sills require us to assign a unique surface to each instance of a sill (sill between units A and B needs to be different from sill of same age and strat codes as one found between E and F). Dykes via cokriging are really hard without just cookie cutting them in (but that is not our problem!). We are not checking for onlap relationships, which can perhaps been seen by having lots of units from one series adjacent to the youngest surface of the older series. Could also think about interpreting these as faults to introduce conceptual uncertainty. All mistakes belong to Mark Jessell, topology code that feeds this system by Vitaliy Ogarko.<br><br>\n",
    "\n",
    "Geology layer needs to have some unique strat code or text, some group code or text<br>\n",
    "Structure layer needs dip/dip direction<br>\n",
    "\n",
    "<font color='red'>Currently hardwired to GSWA 500K map so needs work...</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio import features\n",
    "%matplotlib inline\n",
    "import sys, os\n",
    "sys.path.insert(0,\"../..\")\n",
    "print(os.getcwd())\n",
    "rootdir=r'C:\\Users\\00073294\\Dropbox\\1_Jupyter_notebooks\\m2m\\\\'\n",
    "os.chdir(rootdir)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## First we test to see if we have access to the online data we need\n",
    "mj: We actually only need to check for the Hawiian data at the moment, but soon this will be important for other online data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "try:\n",
    "    import httplib\n",
    "except:\n",
    "    import http.client as httplib\n",
    "\n",
    "def have_access(url):\n",
    "    conn = httplib.HTTPConnection(url, timeout=5)\n",
    "    try:\n",
    "        conn.request(\"HEAD\", \"/\")\n",
    "        conn.close()\n",
    "        display(Image.open(\".\\\\graphics\\\\yes.png\"))\n",
    "        print(\"available: \"+url)\n",
    "    except:\n",
    "        conn.close()\n",
    "        display(Image.open(\".\\\\graphics\\\\no.png\"))\n",
    "        print(\"NOT available: \"+url)\n",
    "\n",
    "#have_access(\"services.slip.wa.gov.au\")\n",
    "have_access(\"oos.soest.hawaii.edu\")\n",
    "#have_access(\"130.95.198.59\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Next we define an area of interest and some other basic stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mname='hams2' #root name of model input data\n",
    "\n",
    "print(os.getcwd())\n",
    "newdir=rootdir+mname\n",
    "print(newdir)\n",
    "os.chdir(newdir)\n",
    "\n",
    "if not os.path.exists(os.path.isdir('tmp')): \n",
    "    os.mkdir('tmp')\n",
    "    os.mkdir('gempy')\n",
    "    os.mkdir('dtm')\n",
    "\n",
    "text=False    #display debug text\n",
    "graphics=True #display debug images\n",
    "\n",
    "\n",
    "model_base=-8200 # top could be found using highest point in topo data\n",
    "\n",
    "minlong=117\n",
    "maxlong=118\n",
    "minlat=-23\n",
    "maxlat=-22\n",
    "\n",
    "minx=500057    #left hams\n",
    "maxx=603028    #right\n",
    "miny=7455348   #bottom\n",
    "maxy=7567953   #top\n",
    "    \n",
    "\n",
    "step_out=0.045 #add (in degrees) so edge pixel from dtm reprojection are not found\n",
    "\n",
    "src_crs = 'epsg:2346'  #input data coord ref system (assumed to be geodetic lat/long WGS84)\n",
    "dst_crs = 'epsg:28350' #model ref system (assumed to be something with metre coordinates)\n",
    "\n",
    "# 'epsg:28350'  =  GDA_1994_MGA_Zone_50 \n",
    "# 'epsg:4326'   =   wgs84 GEODETIC LAT/LONG \n",
    "\n",
    "orientation_decimate=0   # decimation of orientaiton data (0 = no decimation)\n",
    "contact_decimate=30   # decimation of contact data (0 = no decimation)\n",
    "\n",
    "gcode='SUPERGROUP'\n",
    "dcode='DIP'\n",
    "ddcode='DIP_DIR'\n",
    "ccode='CODE'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Download and reproject the appropriate SRTM data\n",
    "mj: Getting this from Hawaii, but could also get from Geoscience Australia (expect when I tried via WCS it blew up for more than 0.25 degree square areas, but I am pretty sure this is a OWS python plugin problem not GA since I can load the whole data with QGIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry import Point\n",
    "\n",
    "bottom=minlat-step_out\n",
    "top=maxlat+step_out\n",
    "left=minlong-step_out\n",
    "right=maxlong+step_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minxll=int(((minlong+180)*120)-step_out)\n",
    "maxxll=int(((maxlong+180)*120)+step_out)\n",
    "minyll=int(((minlat+90)*120)-step_out)\n",
    "maxyll=int(((maxlat+90)*120)+step_out)\n",
    "\n",
    "sizex=round(maxxll-minxll+1)\n",
    "sizey=round(maxyll-minyll+1)\n",
    "print(sizex,sizey)\n",
    "minxll=str(minxll)\n",
    "maxxll=str(maxxll)\n",
    "minyll=str(minyll)\n",
    "maxyll=str(maxyll)\n",
    "bbox=\"[\"+minyll+\":1:\"+maxyll+\"][\"+minxll+\":1:\"+maxxll+\"]\"\n",
    "print\n",
    "link = \"http://oos.soest.hawaii.edu/thredds/dodsC/srtm30plus_v11_land.ascii?elev\"+bbox\n",
    "\n",
    "print (link)\n",
    "f = urlopen(link)\n",
    "myfile = f.read()\n",
    "myfile2=myfile.decode(\"utf-8\") \n",
    "data=myfile2.split(\"---------------------------------------------\")\n",
    "import re\n",
    "\n",
    "grid=re.sub('\\[.*\\]','',data[1]).replace(\",\",\"\").replace(\"elev.elev\",\"\").replace(\"\\n\",\" \").replace(\"  \",\" \")\n",
    "#print(grid)\n",
    "grid=grid.split(\" \")\n",
    "grid=grid[2:(sizex*sizey)+2]\n",
    "OPeNDAP = np.ones((sizey,sizex), dtype='int16')\n",
    "k=0\n",
    "for j in range (0, sizey, 1):\n",
    "    for i in range (0, sizex, 1):\n",
    "        OPeNDAP[sizey-1-j][i]=int(float(grid[k]))\n",
    "        k+=1\n",
    "\n",
    "maxtopo=np.amax(OPeNDAP)\n",
    "print(\"max height =\",maxtopo)\n",
    "maxtopo=np.around(maxtopo,-2)+100\n",
    "print(\"rounded up max height =\",maxtopo)\n",
    "\n",
    "\n",
    "if(graphics):\n",
    "    plt.imshow(OPeNDAP)\n",
    "if(text):\n",
    "    print(myfile)\n",
    "\n",
    "\n",
    "transform = from_origin(minlong, maxlat,0.008333333,0.008333333)\n",
    "\n",
    "new_dataset = rasterio.open('./dtm/'+mname+'_dtm.tif', 'w', driver='GTiff',\n",
    "                            height = OPeNDAP.shape[0], width = OPeNDAP.shape[1],\n",
    "                            count=1, dtype=str(OPeNDAP.dtype),\n",
    "                            crs='+proj=longlat',\n",
    "                            transform=transform)\n",
    "\n",
    "new_dataset.write(OPeNDAP, 1)\n",
    "new_dataset.close()\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "\n",
    "\n",
    "with rasterio.open('./dtm/'+mname+'_dtm.tif') as src:\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "    kwargs = src.meta.copy()\n",
    "    kwargs.update({\n",
    "        'crs': dst_crs,\n",
    "        'transform': transform,\n",
    "        'width': width,\n",
    "        'height': height\n",
    "    })\n",
    "\n",
    "    with rasterio.open('./dtm/'+mname+'_dtm_rp.tif', 'w', **kwargs) as dst:\n",
    "        for i in range(1, src.count + 1):\n",
    "            reproject(\n",
    "                source=rasterio.band(src, i),\n",
    "                destination=rasterio.band(dst, i),\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=transform,\n",
    "                dst_crs=dst_crs,\n",
    "                resampling=Resampling.nearest)\n",
    "            \n",
    "with rasterio.open('./dtm/'+mname+'_dtm.tif') as dataset:\n",
    "\n",
    "    # Read the dataset's valid data mask as a ndarray.\n",
    "    mask = dataset.dataset_mask()\n",
    "    for geom, val in rasterio.features.shapes(\n",
    "                mask, transform=dataset.transform):\n",
    "\n",
    "            # Transform shapes from the dataset's own coordinate\n",
    "            # reference system to 28350.\n",
    "            geom_rp = rasterio.warp.transform_geom(\n",
    "                dataset.crs, dst_crs, geom, precision=6)\n",
    "\n",
    "            # Print GeoJSON shapes to stdout.\n",
    "            print(geom_rp)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Load stratigraphy graph and create list of series (aka groups)\n",
    "mj: The choice of what constitutes basic unit and what a group of units is hard-wired at the moment, but could be altered to any pair. Not even sure we need two levels but it seemed like a good idea at the time. Note that this needs the arcgis plugin version of the topology code (for now) as it seperates the different sub graphs. Text outputs list alternate topologies for series and surfaces, which if confirmed by comapring max-min ages will be a nice source of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.read_gml('.\\\\graph\\\\'+mname+'_strat.gml',label='id') # load a stratigraphy with groups needs to feed via yed!!\n",
    "#print(nx.info(G, n=None))\n",
    "\n",
    "glabels = {}\n",
    "groups=0\n",
    "nlist=list(G.nodes)\n",
    "for n in nlist: # Find out total number of groups and their names groups\n",
    "    if('isGroup' in G.nodes[n]):\n",
    "        groups+=1\n",
    "        glabels[n]=G.nodes[n]['LabelGraphics']['text'].replace(\" \",\"_\")\n",
    "        print(G.nodes[n]['LabelGraphics']['text'].replace(\" \",\"_\"))\n",
    "        \n",
    "print(glabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"groups:\",groups)\n",
    "\n",
    "for p in glabels: #process each group, removing nodes that are not part of that group, and other groups\n",
    "    GD=G.copy() #temporary copy of full graph\n",
    "    print()\n",
    "    print(p,glabels[p],\"----------------------\")\n",
    "    nlist=list(G.nodes)\n",
    "    for n in nlist: # Calculate total number of groups and their names groups\n",
    "        if('gid' in GD.nodes[n]): #normal node\n",
    "            if(GD.nodes[n]['gid']!=p): #normal node but not part of current group\n",
    "                GD.remove_node(n)\n",
    "        else:                     #group node\n",
    "            GD.remove_node(n)\n",
    "    \n",
    "    labels = {}\n",
    "    for node in GD.nodes():   #local store of node labels     \n",
    "        labels[node] = G.nodes[node]['LabelGraphics']['text'].replace(\" \",\"_\")\n",
    "    \n",
    "    plt.figure(p+1) #display strat graph for one group\n",
    "    plt.title(glabels[p])\n",
    "    nx.draw_networkx(GD, pos=nx.kamada_kawai_layout(GD), arrows=True,with_labels=False)\n",
    "    nx.draw_networkx_labels(GD,pos=nx.kamada_kawai_layout(GD), labels=labels, font_size=12,font_family='sans-serif')\n",
    "    \n",
    "    nlist=list(nx.all_topological_sorts(GD)) #all possible sorted directional graphs\n",
    "    f = open('.\\\\tmp\\\\'+glabels[p]+'.txt', 'w')\n",
    "\n",
    "    print(\"choices:\",len(nlist))\n",
    "    f.write(str(len(nlist))+\" \")\n",
    "    f.write(str(len(GD))+\"\\n\")\n",
    "    for m in range(len(nlist)): #process all sorted graphs\n",
    "        for n in range(0,len(GD)): #display nodes for one sorted graph\n",
    "            print(nlist[m][n],G.nodes[nlist[m][n]]['LabelGraphics']['text'])\n",
    "            f.write(G.nodes[nlist[m][n]]['LabelGraphics']['text']+\"\\n\")\n",
    "        if(m<len(nlist)-1):\n",
    "            print(\"....\")\n",
    "    f.close()\n",
    "           \n",
    "Gp=nx.Graph().to_directed() #New Group graph\n",
    "print()\n",
    "\n",
    "nlist=list(G.nodes)\n",
    "for n in nlist: # Find out total number of groups and their names groups\n",
    "    if('isGroup' in G.nodes[n]):\n",
    "        G.add_nodes_from([n])\n",
    "        #print(n)\n",
    "\n",
    "for e in G.edges:\n",
    "    if(G.nodes[e[0]]['gid']!=G.nodes[e[1]]['gid']):\n",
    "        #print(G.nodes[e[0]]['gid'],G.nodes[e[1]]['gid'])\n",
    "        Gp.add_edge(G.nodes[e[0]]['gid'],G.nodes[e[1]]['gid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Create possible stratigraphy sets per group\n",
    "mj: <font color='red'>Uses first of each possible set of toplogies per unit and per group, which is arbitrary. </font>On the other hand we are not checking relative ages again to see if this helps reduce ambiguity, which I think it would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GpD=Gp.copy() #temporary copy of full graph\n",
    "GpD2=Gp.copy() #temporary copy of full graph\n",
    "\n",
    "for e in GpD2.edges: #remove duplicate edges with opposite directions\n",
    "    for f in GpD.edges:\n",
    "        if(e[0]==f[1] and e[1]==f[0] and e[0]<f[0]): #arbitrary choice to ensure edge is not completely removed\n",
    "            Gp.remove_edge(e[0],e[1])\n",
    "\n",
    "plt.figure(groups+1) #display strat graph for one group\n",
    "plt.title(\"groups\")\n",
    "nx.draw_networkx(Gp, pos=nx.kamada_kawai_layout(Gp), arrows=True,with_labels=False)\n",
    "nx.draw_networkx_labels(Gp,pos=nx.kamada_kawai_layout(Gp), labels=glabels, font_size=12,font_family='sans-serif')\n",
    "\n",
    "glist=list(nx.all_topological_sorts(Gp)) #all possible sorted directional graphs    \n",
    "print(\"group choices:\",len(glist))\n",
    "print(glist)\n",
    "nx.write_gml(Gp, '.\\\\tmp\\\\'+mname+'_groups.gml')\n",
    "plt.show()\n",
    "\n",
    "f = open('.\\\\tmp\\\\'+mname+'_groups.txt', 'w')\n",
    "f.write(str(len(glist))+\" \")\n",
    "f.write(str(len(glist[0]))+\"\\n\")\n",
    "\n",
    "for n in range(0,len(glist)):\n",
    "    for m in range(0,len(glist[0])):    \n",
    "        f.write(str(glabels[glist[0][m]])+\"\\n\") #check underscore\n",
    "f.close()\n",
    "\n",
    "\n",
    "g=open('.\\\\tmp\\\\'+mname+'_groups.txt',\"r\")\n",
    "contents =g.readlines()\n",
    "g.close\n",
    "hdr=contents[0].split(\" \")\n",
    "\n",
    "ag=open('.\\\\tmp\\\\'+mname+'_all_sorts.txt',\"w\")\n",
    "for i in range(1,int(hdr[1])+1):\n",
    "    f=open('.\\\\tmp\\\\'+contents[i].replace(\"\\n\",\"\")+\".txt\",\"r\")#check underscore\n",
    "    ucontents =f.readlines()\n",
    "    f.close\n",
    "    uhdr=ucontents[0].split(\" \")\n",
    "    for j in range(1,int(uhdr[1])+1):\n",
    "        print(ucontents[j].replace(\"\\n\",\"\"))\n",
    "        ag.write(ucontents[j].replace(\"\\n\",\"\")+\"\\n\")\n",
    "ag.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Load geology & structure data\n",
    "Currently loading from local files, but could load geology from WFS server at GSWA EXCEPT that the WFS online map has less fields that the zipped shapefiles. Go figure. We don't use fault layer at the moment (except for Vitaliy's topology code) but same logic applies in terms of where to get it from. Already have fault/strat relationships and once we have fault/fault relationships will start to include faults in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract point data from WAROX & geology map for gempy\n",
    "##First we readin the WAROX and map from shapefiles, or wherever...\n",
    "\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "os.environ[\"PROJ_LIB\"] = r\"C:\\Users\\00073294\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\pyproj\\proj_dir\\share\\proj\"\n",
    "\n",
    "\n",
    "geology = gpd.read_file('.\\\\data\\\\'+mname+'_geol.shp')\n",
    "print(geology.crs)\n",
    "\n",
    "#crs = {'init': 'epsg:4326'}\n",
    "\n",
    "\n",
    "#print(geology.crs)\n",
    "base=geology.plot(column=ccode,figsize=(10,10),edgecolor='#000000',linewidth=0.2)\n",
    "#polygon.plot(ax=base, color='none',edgecolor='black')\n",
    "\n",
    "warox = gpd.read_file('.\\\\data\\\\'+mname+'_structure.shp')\n",
    "warox.crs=dst_crs\n",
    "\n",
    "sub_pts = warox[['geometry',dcode,ddcode]]\n",
    "\n",
    "sub_pts.plot(ax=base, color='none',edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Useful code for clipping vector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/earthlab/earthpy/blob/master/earthpy/clip.py\n",
    "\"\"\"\n",
    "earthpy.clip\n",
    "============\n",
    "A module to clip vector data using GeoPandas.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "def _clip_points(shp, clip_obj):\n",
    "    \"\"\"Clip point geometry to the clip_obj GeoDataFrame extent.\n",
    "    Clip an input point GeoDataFrame to the polygon extent of the clip_obj\n",
    "    parameter. Points that intersect the clip_obj geometry are extracted with\n",
    "    associated attributes and returned.\n",
    "    Parameters\n",
    "    ----------\n",
    "    shp : GeoDataFrame\n",
    "        Composed of point geometry that is clipped to clip_obj.\n",
    "    clip_obj : GeoDataFrame\n",
    "        Reference polygon for clipping.\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        The returned GeoDataFrame is a subset of shp that intersects\n",
    "        with clip_obj.\n",
    "    \"\"\"\n",
    "    poly = clip_obj.geometry.unary_union\n",
    "    return shp[shp.geometry.intersects(poly)]\n",
    "\n",
    "\n",
    "def _clip_multi_point(shp, clip_obj):\n",
    "    \"\"\"Clip multi point features to the clip_obj GeoDataFrame extent.\n",
    "    Clip an input multi point to the polygon extent of the clip_obj\n",
    "    parameter. Points that intersect the clip_obj geometry are\n",
    "    extracted with associated attributes returned.\n",
    "    Parameters\n",
    "    ----------\n",
    "    shp : GeoDataFrame\n",
    "        multipoint geometry that is clipped to clip_obj.\n",
    "    clip_obj : GeoDataFrame\n",
    "        Reference polygon for clipping.\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        The returned GeoDataFrame is a clipped subset of shp\n",
    "        containing multi-point and point features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Explode multi-point features when clipping then recreate geom\n",
    "    clipped = _clip_points(shp.explode().reset_index(level=[1]), clip_obj)\n",
    "    clipped = clipped.dissolve(by=[clipped.index]).drop(columns=\"level_1\")[\n",
    "        shp.columns.tolist()\n",
    "    ]\n",
    "\n",
    "    return clipped\n",
    "\n",
    "\n",
    "def _clip_line_poly(shp, clip_obj):\n",
    "    \"\"\"Clip line and polygon geometry to the clip_obj GeoDataFrame extent.\n",
    "    Clip an input line or polygon to the polygon extent of the clip_obj\n",
    "    parameter. Lines or Polygons that intersect the clip_obj geometry are\n",
    "    extracted with associated attributes and returned.\n",
    "    Parameters\n",
    "    ----------\n",
    "    shp : GeoDataFrame\n",
    "        Line or polygon geometry that is clipped to clip_obj.\n",
    "    clip_obj : GeoDataFrame\n",
    "        Reference polygon for clipping.\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        The returned GeoDataFrame is a clipped subset of shp\n",
    "        that intersects with clip_obj.\n",
    "    \"\"\"\n",
    "    # Create a single polygon object for clipping\n",
    "    poly = clip_obj.geometry.unary_union\n",
    "    spatial_index = shp.sindex\n",
    "\n",
    "    # Create a box for the initial intersection\n",
    "    bbox = poly.bounds\n",
    "    # Get a list of id's for each object that overlaps the bounding box and\n",
    "    # subset the data to just those lines\n",
    "    sidx = list(spatial_index.intersection(bbox))\n",
    "    shp_sub = shp.iloc[sidx]\n",
    "\n",
    "    # Clip the data - with these data\n",
    "    clipped = shp_sub.copy()\n",
    "    clipped[\"geometry\"] = shp_sub.intersection(poly)\n",
    "\n",
    "    # Return the clipped layer with no null geometry values\n",
    "    return clipped[clipped.geometry.notnull()]\n",
    "\n",
    "\n",
    "def _clip_multi_poly_line(shp, clip_obj):\n",
    "    \"\"\"Clip multi lines and polygons to the clip_obj GeoDataFrame extent.\n",
    "    Clip an input multi line or polygon to the polygon extent of the clip_obj\n",
    "    parameter. Lines or Polygons that intersect the clip_obj geometry are\n",
    "    extracted with associated attributes and returned.\n",
    "    Parameters\n",
    "    ----------\n",
    "    shp : GeoDataFrame\n",
    "        multiLine or multipolygon geometry that is clipped to clip_obj.\n",
    "    clip_obj : GeoDataFrame\n",
    "        Reference polygon for clipping.\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        The returned GeoDataFrame is a clipped subset of shp\n",
    "        that intersects with clip_obj.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clip multi polygons\n",
    "    clipped = _clip_line_poly(shp.explode().reset_index(level=[1]), clip_obj)\n",
    "\n",
    "    lines = clipped[\n",
    "        (clipped.geometry.type == \"MultiLineString\")\n",
    "        | (clipped.geometry.type == \"LineString\")\n",
    "    ]\n",
    "    line_diss = lines.dissolve(by=[lines.index]).drop(columns=\"level_1\")\n",
    "\n",
    "    polys = clipped[clipped.geometry.type == \"Polygon\"]\n",
    "    poly_diss = polys.dissolve(by=[polys.index]).drop(columns=\"level_1\")\n",
    "\n",
    "    return gpd.GeoDataFrame(\n",
    "        pd.concat([poly_diss, line_diss], ignore_index=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def clip_shp(shp, clip_obj):\n",
    "    \"\"\"Clip points, lines, or polygon geometries to the clip_obj extent.\n",
    "    Both layers must be in the same Coordinate Reference System (CRS) and will\n",
    "    be clipped to the full extent of the clip object.\n",
    "    If there are multiple polygons in clip_obj,\n",
    "    data from shp will be clipped to the total boundary of\n",
    "    all polygons in clip_obj.\n",
    "    Parameters\n",
    "    ----------\n",
    "    shp : GeoDataFrame\n",
    "          Vector layer (point, line, polygon) to be clipped to clip_obj.\n",
    "    clip_obj : GeoDataFrame\n",
    "          Polygon vector layer used to clip shp.\n",
    "          The clip_obj's geometry is dissolved into one geometric feature\n",
    "          and intersected with shp.\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "         Vector data (points, lines, polygons) from shp clipped to\n",
    "         polygon boundary from clip_obj.\n",
    "    Examples\n",
    "    --------\n",
    "    Clipping points (glacier locations in the state of Colorado) with\n",
    "    a polygon (the boundary of Rocky Mountain National Park):\n",
    "        >>> import geopandas as gpd\n",
    "        >>> import earthpy.clip as cl\n",
    "        >>> from earthpy.io import path_to_example\n",
    "        >>> rmnp = gpd.read_file(path_to_example('rmnp.shp'))\n",
    "        >>> glaciers = gpd.read_file(path_to_example('colorado-glaciers.geojson'))\n",
    "        >>> glaciers.shape\n",
    "        (134, 2)\n",
    "        >>> rmnp_glaciers = cl.clip_shp(glaciers, rmnp)\n",
    "        >>> rmnp_glaciers.shape\n",
    "        (36, 2)\n",
    "    Clipping a line (the Continental Divide Trail) with a\n",
    "    polygon (the boundary of Rocky Mountain National Park):\n",
    "        >>> cdt = gpd.read_file(path_to_example('continental-div-trail.geojson'))\n",
    "        >>> rmnp_cdt_section = cl.clip_shp(cdt, rmnp)\n",
    "        >>> cdt['geometry'].length > rmnp_cdt_section['geometry'].length\n",
    "        0    True\n",
    "        dtype: bool\n",
    "    Clipping a polygon (Colorado counties) with another polygon\n",
    "    (the boundary of Rocky Mountain National Park):\n",
    "        >>> counties = gpd.read_file(path_to_example('colorado-counties.geojson'))\n",
    "        >>> counties.shape\n",
    "        (64, 13)\n",
    "        >>> rmnp_counties = cl.clip_shp(counties, rmnp)\n",
    "        >>> rmnp_counties.shape\n",
    "        (4, 13)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shp.geometry\n",
    "        clip_obj.geometry\n",
    "    except AttributeError:\n",
    "        raise AttributeError(\n",
    "            \"Please make sure that your input and clip GeoDataFrames have a\"\n",
    "            \" valid geometry column\"\n",
    "        )\n",
    "\n",
    "    if not any(shp.intersects(clip_obj.unary_union)):\n",
    "        raise ValueError(\"Shape and crop extent do not overlap.\")\n",
    "\n",
    "    if any(shp.geometry.type == \"MultiPoint\"):\n",
    "        return _clip_multi_point(shp, clip_obj)\n",
    "    elif any(shp.geometry.type == \"Point\"):\n",
    "        return _clip_points(shp, clip_obj)\n",
    "    elif any(shp.geometry.type == \"MultiPolygon\") or any(\n",
    "        shp.geometry.type == \"MultiLineString\"\n",
    "    ):\n",
    "        return _clip_multi_poly_line(shp, clip_obj)\n",
    "    else:\n",
    "        return _clip_line_poly(shp, clip_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Useful code for exploding MultiPolygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gist.github.com/mhweber/cf36bb4e09df9deee5eb54dc6be74d26\n",
    "    \n",
    "import geopandas as gpd\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "\n",
    "def explode(indf):\n",
    "#    indf = gpd.GeoDataFrame.from_file(indata)\n",
    "    outdf = gpd.GeoDataFrame(columns=indf.columns)\n",
    "    for idx, row in indf.iterrows():\n",
    "        if type(row.geometry) == Polygon:\n",
    "            outdf = outdf.append(row,ignore_index=True)\n",
    "        if type(row.geometry) == MultiPolygon:\n",
    "            multdf = gpd.GeoDataFrame(columns=indf.columns)\n",
    "            recs = len(row.geometry)\n",
    "            multdf = multdf.append([row]*recs,ignore_index=True)\n",
    "            for geom in range(recs):\n",
    "                multdf.loc[geom,'geometry'] = row.geometry[geom]\n",
    "            outdf = outdf.append(multdf,ignore_index=True)\n",
    "    return outdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Clip map & structure points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left=minx\n",
    "right=maxx\n",
    "bottom=miny\n",
    "top=maxy\n",
    "\n",
    "print(left,right,bottom,top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox=str(bottom)+\",\"+str(left)+\",\"+str(top)+\",\"+str(right)\n",
    "lat_point_list = [bottom, bottom, top, top, bottom]\n",
    "lon_point_list = [left, right, right, left, left]\n",
    "print(bbox)\n",
    "bbox_geom = Polygon(zip(lon_point_list, lat_point_list))\n",
    "\n",
    "polyg = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "print(polyg.geometry.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(geology.crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geology = explode(geology)\n",
    "geology.crs = dst_crs\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "warox_code = gpd.sjoin(sub_pts, geology, how=\"left\", op=\"within\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(warox_code)):\n",
    "    if(str(warox_code[i:i+1][ccode].item())=='A-b-PRK'):\n",
    "        print(str(warox_code[i:i+1][gcode].item()))\n",
    "    if(str(warox_code[i:i+1][gcode].item())=='None' or str(warox_code[i:i+1][gcode].item())=='TOL' ):\n",
    "        warox_code.set_value(i, gcode, warox_code[i:i+1][ccode].item())\n",
    "        print(\"...\",warox_code[i:i+1][gcode].item())\n",
    "        \n",
    "\n",
    "display(warox_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bbox=str(miny)+\",\"+str(minx)+\",\"+str(maxy)+\",\"+str(maxx)\n",
    "y_point_list = [miny, miny, maxy, maxy, miny]\n",
    "x_point_list = [minx, maxx, maxx, minx, minx]\n",
    "print(bbox)\n",
    "bbox_geom = Polygon(zip(x_point_list, y_point_list))\n",
    "\n",
    "polygo = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "\n",
    "geol_clip=clip_shp(geology, polygo)\n",
    "#geol_clip=gpd.overlay(geology, polygo, how='intersection')\n",
    "base = geol_clip.plot(column=ccode,figsize=(7,7),edgecolor='#000000',linewidth=0.2)\n",
    "\n",
    "warox_clip = clip_shp(warox_code, polygo)\n",
    "warox_clip.plot(ax=base, color='none',edgecolor='black')\n",
    "geol_clip.to_file(\"data/geol_clip.shp\")\n",
    "warox_clip.to_file(\"data/warox_clip.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Export orientation data in gempy import format\n",
    "mj: Orientation data needs calculated height as file does not provide it, taken from SRTM data already downloaded. To calculate polarity <font color='red'>(WHICH WE DON'T DO YET)</font> we can calculate the dot product of the dip direction of a bedding plane and the vector to that points nearest basal contact node, if  abs(acos(dot product))>90  then right way up :\n",
    "\n",
    "<img src='./graphics/polarity.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import geometry\n",
    "import requests\n",
    "import rasterio\n",
    "\n",
    "dataset = rasterio.open('.\\\\dtm\\\\'+mname+'_dtm_rp.tif')\n",
    "print(dataset.crs)\n",
    "print(dataset.width)\n",
    "print(dataset.height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "f=open('.\\\\gempy\\\\'+mname+'_orientations.txt',\"w\")\n",
    "f.write(\"X,Y,Z,azimuth,dip,polarity,formation\\n\")\n",
    "for apoint in warox_code.iterrows():\n",
    "    if(apoint[1][dcode]==0):\n",
    "        continue\n",
    "    elif(orientation_decimate!=0): #decimate to reduce number of points\n",
    "        if(i%orientation_decimate!=0):\n",
    "            continue\n",
    "    locations=[(apoint[1]['geometry'].x, apoint[1]['geometry'].y)]\n",
    "    #print(locations)\n",
    "    if(apoint[1]['geometry'].x > minx and apoint[1]['geometry'].x < maxx and  \n",
    "       apoint[1]['geometry'].y > miny and apoint[1]['geometry'].y < maxy):       \n",
    "            for val in dataset.sample(locations):\n",
    "                height=str(val).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "            ostr=str(apoint[1]['geometry'].x)+\",\"+str(apoint[1]['geometry'].y)+\",\"+height+\",\"+str(apoint[1][ddcode])+\",\"+str(apoint[1][dcode])+\",1,\"+str(apoint[1][ccode])+\"\\n\"\n",
    "            f.write(ostr)\n",
    "            i+=1\n",
    "    \n",
    "f.close()\n",
    "\n",
    "## needs polarity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, mapping\n",
    "from shapely.geometry import Polygon, Point, LinearRing\n",
    "import math # or numpy\n",
    "from math import sin,cos\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "def euclidean_distance( l1,l2):\n",
    "    return math.sqrt((l2[0]-l1[0])**2 + (l2[1]-l1[1])**2)\n",
    "contact_decimatex=15\n",
    "\n",
    "geol_clipx=explode(geol_clip)\n",
    "d2r=3.1415927/180.0\n",
    "\n",
    "geol_clipx=explode(geol_clip)\n",
    "i=0\n",
    "for apwrx in warox_code.iterrows():\n",
    "    wpoint = Point(apwrx[1]['geometry'].x,apwrx[1]['geometry'].y)    \n",
    "    for ageol in geol_clipx.iterrows(): # a polygon\n",
    "        apoly=Polygon(ageol[1]['geometry'])\n",
    "        if(not ageol[1]['OBJECTID_1']==41):\n",
    "            continue\n",
    "        if(i%contact_decimatex==0):\n",
    "            close= nearest_points(apoly,wpoint)\n",
    "            print(apwrx[1]['DIP_DIR'],apwrx[1]['DIP'],\"closest point is\",close[0].coords[0][0],close[0].coords[0][1],wpoint)\n",
    "            l=sin(d2r*float(apwrx[1]['DIP_DIR']))*cos(d2r*(90.0-float(apwrx[1]['DIP'])))\n",
    "            m=cos(d2r*float(apwrx[1]['DIP_DIR']))*cos(d2r*(90.0-float(apwrx[1]['DIP'])))\n",
    "            n=-sin(d2r*(90.0-float(apwrx[1]['DIP'])))   \n",
    "            print(i,l,m,n)\n",
    "    i+=1\n",
    "    break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Find those series that don't have any orientation or contact point data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('.\\\\tmp\\\\'+mname+'_groups.txt',\"r\")\n",
    "contents =f.readlines()\n",
    "f.close\n",
    "\n",
    "ngroups=contents[0].split(\" \")\n",
    "ngroups=int(ngroups[1])\n",
    "\n",
    "groups=[]\n",
    "for i in range (1,int(ngroups)+1):\n",
    "    #print(contents[i].replace(\"\\n\",\"\"))\n",
    "    groups.append((contents[i].replace(\"\\n\",\"\"),0))\n",
    "\n",
    "print(ngroups,groups)\n",
    "\n",
    "for i in range (0,ngroups):\n",
    "    for apoint in warox_code.iterrows():\n",
    "        agroup=apoint[1][gcode]\n",
    "        #print(agroup)\n",
    "        if(groups[i][0]==agroup.replace(\" \",\"_\")):\n",
    "            lgroups=list(groups[i])\n",
    "            lgroups[1]=1\n",
    "            lgroups=tuple(lgroups)\n",
    "            groups[i]=lgroups\n",
    "\n",
    "print(\"Orientations----------\\n\",ngroups,groups)\n",
    "\n",
    "for i in range (0,ngroups):\n",
    "    for apoly in geology.iterrows():\n",
    "        agroup=apoint[1][gcode]\n",
    "        #print(agroup)\n",
    "        if(groups[i][0]==agroup.replace(\" \",\"_\")):\n",
    "            lgroups=list(groups[i])\n",
    "            lgroups[1]=1\n",
    "            lgroups=tuple(lgroups)\n",
    "            groups[i]=lgroups\n",
    "\n",
    "all_codes=[]\n",
    "for ageol in geol_clip.iterrows(): # central polygon\n",
    "        all_codes.append(ageol[1][ccode])\n",
    "\n",
    "print(\"Contacts----------\\n\",len(set(all_codes)),set(all_codes))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Create arbitrary point for series with no orientation data\n",
    "Not sure if gempy needs this but geomodeller does. <font color='red'>Currently just gives a point dipping 45 degrees to North, but could use dip direction normal to basal surface)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import shape, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "f=open('.\\\\gempy\\\\'+mname+'_orientations.txt',\"a\")\n",
    "\n",
    "for i in range (0,ngroups):\n",
    "    if(groups[i][1]==0):\n",
    "        for ageol in geol_clip.iterrows():\n",
    "            if(ageol[1][ccode]==groups[i][0] and groups[i][1]==0):\n",
    "                apoly=Polygon(ageol[1]['geometry'])\n",
    "                apoint=apoly.representative_point()\n",
    "                #print(apoint.x,apoint.y)\n",
    "                locations=[(apoint.x,apoint.y)]\n",
    "                for val in dataset.sample(locations):\n",
    "                    height=str(val).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "                ostr=str(apoint.x)+\",\"+str(apoint.y)+\",\"+height+\",0,45,1\"+\",\"+str(ageol[1][ccode])+\"\\n\"\n",
    "                f.write(ostr)\n",
    "                plt.title(str(ageol[1][ccode]))\n",
    "                plt.scatter(apoint.x,apoint.y,color=\"red\")\n",
    "                plt.plot(*apoly.exterior.xy)\n",
    "                plt.show()\n",
    "                break\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Export contact information subset of each polygon to gempy format\n",
    "mj: Orientation data needs calculated height as file does not provide it, taken from SRTM data already downloaded. Need to reduce number of points whilst retaining useful info (Ranee's job!)'\n",
    "To calculate which are the basal units contact for a polygon find the polygons which are older than the selected polygon, in the example below the central polygon has relative age 23 so its basal contact is with the polygons whose ages are 26 & 28. If there are no older units for a polygon it has no basal content. We keep every nth node based on the decimate term (simple count along polyline). gempy seems to need at least two points per surface, so we always take the first two points.\n",
    "\n",
    "<font color='red'>Needs also to account for case when missing surface in stratigraphy means that locally basal polyline can ALSO be anothe rthan youngest surface.</font>\n",
    "<img src='./graphics/base.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from shapely.ops import snap\n",
    "#newwd=\"C:\\\\Users\\\\00073294\\Dropbox\\\\1_Jupyter_notebooks\\\\map2model\\\\\"\n",
    "#os.chdir(newwd)\n",
    "\n",
    "## Reproject topography to approriate metre-based CRS\n",
    "           \n",
    "dataset = rasterio.open('.\\\\dtm\\\\'+mname+'_dtm_rp.tif')\n",
    "ag=open('.\\\\tmp\\\\'+mname+'_all_sorts.txt',\"r\")\n",
    "contents =ag.readlines()\n",
    "ag.close\n",
    "print(\"surfaces:\",len(contents))\n",
    "print(\"polygons:\",len(geol_clip))\n",
    "ulist=[]\n",
    "for i in range(0,len(contents)):\n",
    "    ulist.append([i, contents[i].replace(\"\\n\",\"\")])\n",
    "print(ulist)\n",
    "\n",
    "\n",
    "ac=open('.\\\\gempy\\\\'+mname+'_contacts.txt',\"w\")\n",
    "ac.write(\"X,Y,Z,formation\\n\")\n",
    "print(dataset.bounds)\n",
    "j=0\n",
    "allpts=0\n",
    "for ageol in geol_clip.iterrows(): # central polygon\n",
    "    neighbours=[]\n",
    "    #print(ageol[1].geometry)\n",
    "    j+=1\n",
    "    out=[item for item in ulist if ageol[1][ccode] in item]\n",
    "    if(len(out)>0):\n",
    "        central=out[0][0]    #relative age of central polygon\n",
    "        central_poly=ageol[1].geometry\n",
    "        for bgeol in geol_clip.iterrows(): #potential neighbouring polygons  \n",
    "            if(ageol[1].geometry!=bgeol[1].geometry): #do not compare with self\n",
    "                if (ageol[1].geometry.intersects(bgeol[1].geometry)): # is a neighbour\n",
    "                    neighbours.append([(bgeol[1][ccode],bgeol[1].geometry)])  \n",
    "\n",
    "        if(len(neighbours) >0):\n",
    "            for i in range (0,len(neighbours)):\n",
    "                out=[item for item in ulist if neighbours[i][0][0] in item]\n",
    "                #print(out)\n",
    "                if(len(out)>0):\n",
    "                    #if(out[0][0] > central and out[0][0] < youngest_older): # neighbour is older than central, and younger than previous candidate\n",
    "                    if(out[0][0] > central ): # neighbour is older than central\n",
    "                        older_polygon=neighbours[i][0][1]\n",
    "                        if(not central_poly.is_valid ):\n",
    "                            central_poly = central_poly.buffer(0)\n",
    "                        if(not older_polygon.is_valid):\n",
    "                            older_polygon = older_polygon.buffer(0)\n",
    "                        LineStringC = central_poly.intersection(older_polygon)\n",
    "                        if(LineStringC.wkt.split(\" \")[0]=='GEOMETRYCOLLECTION' or \n",
    "                           LineStringC.wkt.split(\" \")[0]=='MULTIPOLYGON' or\n",
    "                           LineStringC.wkt.split(\" \")[0]=='POLYGON'): #ignore polygon intersections for now, worry about them later!\n",
    "                             continue                      \n",
    "                        elif(LineStringC.wkt.split(\" \")[0]=='GEOMETRYCOLLECTION' or LineStringC.wkt.split(\" \")[0]=='MULTILINESTRING'):\n",
    "                            i=0\n",
    "                            for lineC in LineStringC: #process all linestrings\n",
    "                                if(contact_decimate!=0): #decimate to reduce number of points\n",
    "                                    if(i%contact_decimate==0 or i==1 or i==2): #decimate to reduce number of points, but also take second and third point of a series to keep gempy happy\n",
    "                                        locations=[(lineC.coords[0][0],lineC.coords[0][1])] #doesn't like point right on edge?\n",
    "                                        #print(i,\"ml\",locations,lineC.coords[0][1], maxy)\n",
    "                                        if(lineC.coords[0][0] > dataset.bounds[0] and lineC.coords[0][0] < dataset.bounds[2] and  \n",
    "                                           lineC.coords[0][1] > dataset.bounds[1] and lineC.coords[0][1] < dataset.bounds[3]):       \n",
    "                                                for val in dataset.sample(locations):\n",
    "                                                    height=str(val).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "                                                ostr=str(lineC.coords[0][0])+\",\"+str(lineC.coords[0][1])+\",\"+height+\",\"+str(ageol[1][ccode])+\"\\n\"\n",
    "                                                ac.write(ostr)\n",
    "                                                allpts+=1\n",
    "                                else:\n",
    "                                    locations=[(lineC.coords[0][0]+0.0000001,lineC.coords[0][1])] #doesn't like point right on edge?\n",
    "                                    if(lineC.coords[0][0] > dataset.bounds[0] and lineC.coords[0][0] < dataset.bounds[2] and  \n",
    "                                        lineC.coords[0][1] > dataset.bounds[1] and lineC.coords[0][1] < dataset.bounds[3]):       \n",
    "                                        for val in dataset.sample(locations):\n",
    "                                            height=str(val).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "                                        ostr=str(lineC.coords[0][0])+\",\"+str(lineC.coords[0][1])+\",\"+height+\",\"+str(ageol[1][ccode])+\"\\n\"\n",
    "                                        ac.write(ostr)\n",
    "                                        allpts+=1\n",
    "\n",
    "                                i+=1\n",
    "                        elif(LineStringC.wkt.split(\" \")[0]=='LINESTRING'): # apparently this is not needed\n",
    "                            i=0\n",
    "                            for pt in LineStringC.coords: #process one linestring\n",
    "                                #if(i%contact_decimate==0): #decimate to reduce number of points\n",
    "                                #print(\"ls\",pt)\n",
    "                                \"\"\"\n",
    "                                locations=[(float(pt[0]),float(pt[1]))]\n",
    "                                for val in dataset.sample(locations):\n",
    "                                    height=str(val).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "                                ostr=str(pt[0])+\",\"+str(pt[1])+\",\"+height+\",\"+str(ageol[1][ccode])+\"\\n\"\n",
    "                                ac.write(ostr)\n",
    "                                \"\"\"\n",
    "                                i+=1\n",
    "                        elif(LineStringC.wkt.split(\" \")[0]=='POINT'): # apparently this is not needed\n",
    "                            #print(\"pt\",LineStringC.coords)\n",
    "                            \"\"\"\n",
    "                            locations=[(float(pt[0]),float(pt[1]))]\n",
    "                            for val in dataset.sample(locations):\n",
    "                                height=str(val).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "                            ostr=str(pt[0])+\",\"+str(pt[1])+\",\"+height+\",\"+str(ageol[1][ccode])+\"\\n\"\n",
    "                            ac.write(ostr)\n",
    "                            \"\"\"\n",
    "                            i+=1\n",
    "                        else:\n",
    "                            #print(LineStringC.wkt.split(\" \")[0]) # apparently this is not needed\n",
    "                            i+=1\n",
    "\n",
    "\n",
    "\n",
    "ac.close()\n",
    "print(\"allpts=\",allpts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Now we define the basic model and load the orientation and lithological information\n",
    "mj: We inset the model dimensions as once reprojects the edge pixels of the topography are undefined. Probably doesn't need to be that extreme (unit is metres). Uses parameter <b>maxtopo</b> from maximum elevtion to define upper bound to Z. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('.\\\\gempy\\\\'+mname+'_contacts.txt',\"r\")\n",
    "contents =f.readlines()\n",
    "f.close\n",
    "\n",
    "all_cpts=[]\n",
    "for cpt in contents[1:]:\n",
    "    cpt2=cpt.split(\",\")[3]\n",
    "    all_cpts.append(cpt2.replace(\"\\n\",\"\"))\n",
    "actual_cpts=set(all_cpts)\n",
    "print(actual_cpts)\n",
    "\n",
    "f=open('.\\\\gempy\\\\'+mname+'_contacts2.txt',\"w\")\n",
    "f.write(contents[0])\n",
    "for cpt in contents[1:]:\n",
    "    cpt2=cpt.split(\",\")[3]\n",
    "    cpt2=cpt2.replace(\"\\n\",\"\")\n",
    "    out=[item for item in actual_cpts if cpt2 in item]\n",
    "    if(len(out)>0):\n",
    "        f.write(cpt)\n",
    "    else:\n",
    "        print(cpt)\n",
    "f.close()\n",
    "\n",
    "f=open('.\\\\gempy\\\\'+mname+'_orientations.txt',\"r\")\n",
    "ocontents =f.readlines()\n",
    "f.close\n",
    "\n",
    "f=open('.\\\\gempy\\\\'+mname+'_orientations2.txt',\"w\")\n",
    "f.write(ocontents[0])\n",
    "for cpt in ocontents[1:]:\n",
    "    cpt2=cpt.split(\",\")[6]\n",
    "    cpt2=cpt2.replace(\"\\n\",\"\")\n",
    "    out=[item for item in actual_cpts if cpt2 in item]\n",
    "    if(len(out)>0):\n",
    "        f.write(cpt)\n",
    "    else:\n",
    "        print(cpt)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
